{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26a29faf-75d0-4f49-b377-c9494a284c26",
   "metadata": {},
   "source": [
    "# Common utility functions\n",
    "\n",
    "This notebook has common utility functions that are independent of whether we're running on datalab-abafar, datalab-bespin or a local computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00643c1-77d3-4123-b967-e8923db5916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import base64\n",
    "from collections import namedtuple, defaultdict\n",
    "from collections.abc import Mapping\n",
    "import functools\n",
    "import random\n",
    "import subprocess\n",
    "import traceback\n",
    "\n",
    "# third party imports\n",
    "import pandas as pd\n",
    "from pyprojroot import here\n",
    "import dask\n",
    "import dask.array as da\n",
    "from dask import delayed\n",
    "import distributed\n",
    "from distributed import LocalCluster\n",
    "from dask.diagnostics import ProgressBar\n",
    "import zarr\n",
    "import fsspec\n",
    "from fsspec.implementations.zip import ZipFileSystem\n",
    "import gcsfs\n",
    "import yaml\n",
    "import requests\n",
    "import malariagen_data\n",
    "import numcodecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40971da-6536-45ba-8121-b53ccbaae7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(*msg):\n",
    "    \"\"\"Simple logging function that writes to stdout and flushes immediately.\"\"\"\n",
    "    print(*msg, file=sys.stdout)\n",
    "    sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca16ffe-848f-4966-8038-16dd5c9a3f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_original_samples(*, sample_set):\n",
    "    \"\"\"Read the original_samples.tsv file for a given sample set.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample_set : str\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # directory containing tracking metadata for the sample set\n",
    "    tracking_dir = here() / 'tracking' / sample_set\n",
    "    \n",
    "    # read into pandas\n",
    "    df_original_samples = pd.read_csv(tracking_dir / 'original_samples.tsv', sep='\\t') \n",
    "    \n",
    "    # add in sample_set\n",
    "    df_original_samples[\"sample_set\"] = sample_set\n",
    "    \n",
    "    return df_original_samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d007ff60-4183-4c8d-a9f5-d4b0660d89b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wgs_derived_samples(*, sample_set):\n",
    "    \"\"\"Read the wgs_derived_samples.tsv file for a given sample set.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample_set : str\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # directory containing tracking metadata for the sample set\n",
    "    tracking_dir = here() / 'tracking' / sample_set\n",
    "    \n",
    "    # read into pandas\n",
    "    df_wgs_derived_samples = pd.read_csv(tracking_dir / 'wgs_derived_samples.tsv', sep='\\t')\n",
    "    \n",
    "    # also read in original samples and merge\n",
    "    df_original_samples = read_original_samples(sample_set=sample_set)\n",
    "    df_wgs_derived_samples = pd.merge(df_wgs_derived_samples, df_original_samples, on=\"original_sample_id\", how=\"left\")\n",
    "    \n",
    "    return df_wgs_derived_samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a4f387-c3b9-4858-930f-f819707f1596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wgs_lanelets(*, sample_set):\n",
    "    \"\"\"Read the wgs_lanelets.tsv file for a given sample set.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample_set : str\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # directory containing tracking metadata for the sample set\n",
    "    tracking_dir = here() / 'tracking' / sample_set\n",
    "    \n",
    "    # read into pandas\n",
    "    df_wgs_lanelets = (\n",
    "        pd.read_csv(tracking_dir / 'wgs_lanelets.tsv', sep='\\t')\n",
    "        .rename(columns={'sample': 'derived_sample_id'})\n",
    "    )\n",
    "\n",
    "    # also read in derived samples and merge\n",
    "    df_wgs_derived_samples = read_wgs_derived_samples(sample_set=sample_set)\n",
    "    df_wgs_lanelets = pd.merge(df_wgs_lanelets, df_wgs_derived_samples, on=\"derived_sample_id\", how=\"left\")\n",
    "    \n",
    "    return df_wgs_lanelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a425bd3e-4550-4471-a609-ff588eac555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wgs_snp_data(*, sample_set):\n",
    "    \"\"\"Read the wgs_snp_data.tsv file for a given sample set.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample_set : str\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # directory containing tracking metadata for the sample set\n",
    "    tracking_dir = here() / 'tracking' / sample_set\n",
    "    \n",
    "    # read into pandas\n",
    "    df_wgs_snp_data = (\n",
    "        pd.read_csv(tracking_dir / 'wgs_snp_data.tsv', sep='\\t')    \n",
    "        .rename(columns={'sample': 'derived_sample_id'})\n",
    "    )        \n",
    "\n",
    "    # also read in derived samples and merge\n",
    "    df_wgs_derived_samples = read_wgs_derived_samples(sample_set=sample_set)\n",
    "    df_wgs_snp_data = pd.merge(df_wgs_snp_data, df_wgs_derived_samples, on=\"derived_sample_id\", how=\"left\")\n",
    "\n",
    "    return df_wgs_snp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63c3bbf-6f79-453e-8f4e-54d5abd268e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_release_config(*, release):\n",
    "    \n",
    "    # New name\n",
    "    file_path = here() / \"tracking\" / \"release\" / release / \"release_sample_sets.yaml\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        # Old name\n",
    "        file_path = file_path = here() / \"tracking\" / \"release\" / release / \"config.yml\"\n",
    "    \n",
    "    with open(file_path, mode=\"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c82845-753b-45dc-a167-a08a17d5b111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_species_group_config():\n",
    "    with open(here() / \"species-group_config.yaml\", mode=\"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return defaultdict(lambda: None, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbfa9c2-5f3e-4ae1-a74f-953123c0a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subprocess_dump(result):\n",
    "    if hasattr(result, 'stdout'):\n",
    "        stdout = result.stdout.decode()\n",
    "        print(stdout, file=sys.stdout)\n",
    "        sys.stdout.flush()\n",
    "    if hasattr(result, 'stderr'):\n",
    "        stderr = result.stderr.decode()\n",
    "        print(stderr, file=sys.stderr)\n",
    "        sys.stderr.flush()\n",
    "\n",
    "\n",
    "def bash(cmd, check=True):\n",
    "    log(cmd)\n",
    "    cmd = \"set -xeuo pipefail; \" + cmd\n",
    "    try:\n",
    "        result = subprocess.run(cmd, \n",
    "                                check=check, \n",
    "                                capture_output=True, \n",
    "                                shell=True, \n",
    "                                executable=\"/bin/bash\")\n",
    "    except Exception as e:\n",
    "        subprocess_dump(e)\n",
    "        return e\n",
    "    else:\n",
    "        subprocess_dump(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014db9a8-07be-4c3e-aacf-345f34eea638",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache(maxsize=None)\n",
    "def http_head(url):\n",
    "    # small optimisation, cache results of http head requests because\n",
    "    # these can be slow, especially if there are a lot of them to do\n",
    "    response = requests.head(url)\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError(f\"status {response.status_code} for {url}\")\n",
    "    return response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f961e8-f7be-408a-b850-c9e70cafd85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delayed_curl_to_gcs(*, gcs, source_url, gcs_url, raw_md5=None, verbose=False, force=False):\n",
    "    \"\"\"Copy an object from an input URL (e.g., Sanger S3) to GCS.\"\"\"\n",
    "\n",
    "    if raw_md5 is not None:\n",
    "\n",
    "        # Convert MD5 to base64-encoded 128-bit MD5 hash\n",
    "        expected_md5 = base64.b64encode(bytes.fromhex(raw_md5)).decode('utf-8')\n",
    "\n",
    "        if not force:\n",
    "            \n",
    "            # Check whether this file already exists\n",
    "            if gcs.exists(gcs_url):\n",
    "\n",
    "                # Check whether the existing file's MD5 hash matches the one specified \n",
    "                file_info = gcs.info(gcs_url)\n",
    "                existing_md5 = file_info['md5Hash']\n",
    "                if existing_md5 == expected_md5:\n",
    "\n",
    "                    # Don't copy this file.\n",
    "                    if verbose:\n",
    "                        log(f'{gcs_url} - skipping, file exists at target location and has expected MD5 and size')\n",
    "                    return None\n",
    "\n",
    "        # Compose the Bash copy command\n",
    "        copy_command = f'curl -s {source_url} | gcloud storage cp -q --content-md5={expected_md5} - {gcs_url}'\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        if not force:\n",
    "\n",
    "            # Check whether this file already exists\n",
    "            if gcs.exists(gcs_url):\n",
    "\n",
    "                # Check file sizes\n",
    "                expected_size = int(http_head(source_url).headers['content-length'])\n",
    "                file_info = gcs.info(gcs_url)\n",
    "                existing_size = file_info['size']\n",
    "                if existing_size == expected_size:\n",
    "\n",
    "                    # Don't copy this file.\n",
    "                    if verbose:\n",
    "                        log(f'{gcs_url} - skipping, file exists at target location and has expected size')\n",
    "                    return None\n",
    "\n",
    "        # Compose the Bash copy command, without MD5 check\n",
    "        copy_command = f'curl -s {source_url} | gsutil -q cp - {gcs_url}'\n",
    "            \n",
    "    # Setup delayed computation\n",
    "    task = delayed(bash)(copy_command)\n",
    "    \n",
    "    return task\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590acedb-0ee7-4fd9-ab9c-8a7f4986eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    \n",
    "    # newer versions of zarr\n",
    "    \n",
    "    from zarr.storage import KVStore\n",
    "    \n",
    "    class SafeStore(KVStore):\n",
    "        \n",
    "        def __getitem__(self, key):\n",
    "            try:\n",
    "                return self._mutable_mapping[key]\n",
    "            except KeyError as e:\n",
    "                # always raise a runtime error to ensure zarr propagates the exception\n",
    "                raise RuntimeError(e)\n",
    "\n",
    "        def __contains__(self, key):\n",
    "            return key in self._mutable_mapping\n",
    "\n",
    "                \n",
    "except ImportError:\n",
    "    \n",
    "    # older versions of zarr\n",
    "\n",
    "    class SafeStore(Mapping):\n",
    "\n",
    "        ## This helps to ensure that no missing data are silently filled in.\n",
    "\n",
    "        def __init__(self, store):\n",
    "            self.store = store\n",
    "\n",
    "        def __getitem__(self, key):\n",
    "            try:\n",
    "                return self.store[key]\n",
    "            except KeyError as e:\n",
    "                # always raise a runtime error to ensure zarr propagates the exception\n",
    "                raise RuntimeError(e)\n",
    "\n",
    "        def __contains__(self, key):\n",
    "            return key in self.store\n",
    "\n",
    "        def __iter__(self):\n",
    "            return iter(self.store)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dd5bdb-4c35-452b-8323-f8fce0ebb5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_gcs_zip_zarr(*, gcs_url, gcs):\n",
    "    \"\"\"Open the zipped zarr for a given GCS URL.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gcs_url : str\n",
    "    gcs : GCSFileSystem\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    zarr_data : zarr.core.Array or zarr.hierarchy.Group\n",
    "    \n",
    "    \"\"\"\n",
    "    zip_file = gcs.open(gcs_url)\n",
    "    zip_fs = ZipFileSystem(zip_file)\n",
    "    zarr_store = SafeStore(zip_fs.get_mapper(\"/\"))\n",
    "    zarr_data = zarr.open(store=zarr_store, mode='r')\n",
    "    \n",
    "    return zarr_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d8c38-c2bb-4ca9-88c6-622aa1af1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_gcs_zarr(*, gcs_url, gcs):\n",
    "    \"\"\"Open the zarr for a given GCS URL.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gcs_url : str\n",
    "    gcs : GCSFileSystem\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    zarr_data : zarr.core.Array or zarr.hierarchy.Group\n",
    "    \n",
    "    \"\"\"\n",
    "    zarr_store = SafeStore(gcs.get_mapper(gcs_url))\n",
    "    zarr_data = zarr.Group(zarr_store, read_only=True)\n",
    "    \n",
    "    return zarr_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d626a92f-9c50-4948-95b0-40af3ab4c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_set_qc_all_pass(*, release, sample_set):\n",
    "    \"\"\"Return pandas Series of values (True or False) for each sample in the sample_set determined by whether all QC filters have \"PASS\" for that sample.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    release : str\n",
    "    sample_set : str\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    s : Series\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    release_dir = here() / \"tracking\" / \"release\" / release\n",
    "    \n",
    "    sequence_qc_df = pd.read_csv(release_dir / \"wgs_sequence_qc\" / f\"sequence_qc_filters_{sample_set}.tsv\", sep=\"\\t\", index_col=0)\n",
    "    replicate_qc_df = pd.read_csv(release_dir / \"wgs_replicate_qc\" / f\"replicate_qc_filters_{sample_set}.tsv\", sep=\"\\t\", index_col=0)\n",
    "    anomaly_qc_df = pd.read_csv(release_dir / \"wgs_population_qc\" / f\"anomaly_qc_filters_{sample_set}.tsv\", sep=\"\\t\", index_col=0)\n",
    "    \n",
    "    qc_dfs = [sequence_qc_df, replicate_qc_df, anomaly_qc_df]\n",
    "    \n",
    "    # We stopped using PCA outlier filters circa Ag3.11\n",
    "    pca_qc_path = release_dir / \"wgs_population_qc\" / f\"pca_qc_filters_{sample_set}.tsv\"\n",
    "    if pca_qc_path.is_file():\n",
    "        pca_qc_df = pd.read_csv(pca_qc_path, sep=\"\\t\", index_col=0)\n",
    "        qc_dfs.append(pca_qc_df)\n",
    "    \n",
    "    joined_df = pd.concat(qc_dfs, axis=1, sort=False)\n",
    "    \n",
    "    return (joined_df == \"PASS\").all(axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global-global-binder-4.1.0",
   "language": "python",
   "name": "conda-env-global-global-binder-4.1.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
