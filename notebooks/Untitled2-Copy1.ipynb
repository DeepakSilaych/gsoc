{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a44a300c-2513-404f-9f73-f498b706121a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import malariagen_data\n",
    "import allel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import joblib\n",
    "import os\n",
    "import concurrent.futures\n",
    "import warnings\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3428bdc3-6ecc-4b88-bbb1-9b1e35771759",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e5648c7-e145-4f42-9719-b6819dd3c683",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "'use strict';\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    function drop(id) {\n",
       "      const view = Bokeh.index.get_by_id(id)\n",
       "      if (view != null) {\n",
       "        view.model.document.clear()\n",
       "        Bokeh.index.delete(view)\n",
       "      }\n",
       "    }\n",
       "\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null) {\n",
       "      drop(id)\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim()\n",
       "            drop(id)\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded(error = null) {\n",
       "    const el = document.getElementById(null);\n",
       "    if (el != null) {\n",
       "      const html = (() => {\n",
       "        if (typeof root.Bokeh === \"undefined\") {\n",
       "          if (error == null) {\n",
       "            return \"BokehJS is loading ...\";\n",
       "          } else {\n",
       "            return \"BokehJS failed to load.\";\n",
       "          }\n",
       "        } else {\n",
       "          const prefix = `BokehJS ${root.Bokeh.version}`;\n",
       "          if (error == null) {\n",
       "            return `${prefix} successfully loaded.`;\n",
       "          } else {\n",
       "            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n",
       "          }\n",
       "        }\n",
       "      })();\n",
       "      el.innerHTML = html;\n",
       "\n",
       "      if (error != null) {\n",
       "        const wrapper = document.createElement(\"div\");\n",
       "        wrapper.style.overflow = \"auto\";\n",
       "        wrapper.style.height = \"5em\";\n",
       "        wrapper.style.resize = \"vertical\";\n",
       "        const content = document.createElement(\"div\");\n",
       "        content.style.fontFamily = \"monospace\";\n",
       "        content.style.whiteSpace = \"pre-wrap\";\n",
       "        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n",
       "        content.textContent = error.stack ?? error.toString();\n",
       "        wrapper.append(content);\n",
       "        el.append(wrapper);\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(() => display_loaded(error), 100);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.6.2.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      try {\n",
       "            for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "\n",
       "      } catch (error) {throw error;\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(null)).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(null);\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.6.2.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {throw error;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(null)).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "ag3 = malariagen_data.Ag3(\n",
    "    sample_sets=\"3.3\",\n",
    "    debug=False,\n",
    "    show_progress=False\n",
    ")\n",
    "\n",
    "samples_metadata = ag3.sample_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bfbdd17-3064-40b7-ad95-7c683a282671",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 500 samples for training and validation.\n"
     ]
    }
   ],
   "source": [
    "gambiae_coluzzii_samples = samples_metadata.query(\"aim_species in ['gambiae', 'coluzzii']\").dropna(subset=[\"sample_id\", \"aim_species\"])\n",
    "num_samples_for_training = 500\n",
    "sub_samples = gambiae_coluzzii_samples.sample(num_samples_for_training, random_state=42)\n",
    "sub_sample_ids = sub_samples['sample_id'].tolist()\n",
    "print(f\"Selected {len(sub_samples)} samples for training and validation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "098ff787-fa01-4104-b762-f6c2a68b7ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_labels_all_samples = sub_samples.set_index('sample_id')['aim_species']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "603f68d6-77a4-40f0-a545-dcb47f184fda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_diploid_allel(gt_array):\n",
    "    \"\"\"Encodes a diploid genotype array into a dosage matrix.\"\"\"\n",
    "    gt = allel.GenotypeArray(gt_array)\n",
    "    allele_counts = gt.to_allele_counts()\n",
    "    alt_allele_dosage = allele_counts[:, :, 1]\n",
    "    dosage_matrix = np.where(gt.is_missing(), np.nan, alt_allele_dosage)\n",
    "    return dosage_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0216f5d3-d5c8-461d-9535-98f90829279d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Loading partitions from partitions_list.json...\n"
     ]
    }
   ],
   "source": [
    "partitions_list_file = 'partitions_list.json'\n",
    "all_partitions_to_process = []\n",
    "partitions_exist = False\n",
    "\n",
    "# Check if the partitions file exists and is not empty\n",
    "if os.path.exists(partitions_list_file) and os.path.getsize(partitions_list_file) > 0:\n",
    "    try:\n",
    "        with open(partitions_list_file, 'r') as f:\n",
    "            all_partitions_to_process = json.load(f)\n",
    "        partitions_exist = True\n",
    "        print(f\"  ✅ Loading partitions from {partitions_list_file}...\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"  ❌ Error decoding {partitions_list_file}. File may be corrupted. Regenerating...\")\n",
    "        partitions_exist = False\n",
    "\n",
    "if not partitions_exist:\n",
    "    print(f\"  🚀 Generating partitions and saving to {partitions_list_file}...\")\n",
    "    contigs = ag3.contigs\n",
    "    partition_window_size = 1_000_000\n",
    "\n",
    "    for contig in contigs:\n",
    "        try:\n",
    "            contig_callset_info = ag3.snp_calls(region=contig, sample_query=f\"sample_id in {sub_sample_ids}\")\n",
    "            if 'variant_position' not in contig_callset_info or contig_callset_info['variant_position'].shape[0] == 0:\n",
    "                continue\n",
    "            max_pos = contig_callset_info['variant_position'].values.max()\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        start_pos = 1\n",
    "        while start_pos <= max_pos:\n",
    "            end_pos = min(start_pos + partition_window_size - 1, max_pos)\n",
    "            all_partitions_to_process.append((contig, int(start_pos), int(end_pos)))\n",
    "            start_pos = end_pos + 1\n",
    "    \n",
    "    with open(partitions_list_file, 'w') as f:\n",
    "        json.dump(all_partitions_to_process, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8f11146-1e3e-484d-9a9e-65f0c981a04a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = 'trained_classifiers'\n",
    "results_dir = 'validation_results'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d0adaa1-1dfe-4b40-bd49-229550f28817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_partition(partition_tuple):\n",
    "    p_contig, p_start, p_end = partition_tuple\n",
    "    partition_id = f\"{p_contig}_{p_start}-{p_end}\"\n",
    "    \n",
    "    model_filename = os.path.join(output_dir, f\"classifier_{partition_id}.joblib\")\n",
    "    results_filename = os.path.join(results_dir, f\"results_{partition_id}.json\")\n",
    "\n",
    "    if os.path.exists(model_filename) and os.path.exists(results_filename):\n",
    "        print(f\"  ✅ Partition {partition_id} already processed. Loading results.\")\n",
    "        with open(results_filename, 'r') as f:\n",
    "            partition_results = json.load(f)\n",
    "        return partition_id, partition_results\n",
    "\n",
    "    print(f\"\\n  🚀 Training for Partition: {partition_id} ---\")\n",
    "    try:\n",
    "        callset_partition = ag3.snp_calls(\n",
    "            region=f\"{p_contig}:{p_start}-{p_end}\", \n",
    "            sample_query=f\"sample_id in {sub_sample_ids}\"\n",
    "        )\n",
    "        \n",
    "        if callset_partition['call_genotype'].shape[0] == 0:\n",
    "            return None\n",
    "\n",
    "        p_call_genotype = callset_partition['call_genotype'].values\n",
    "        p_missing_genotype_mask = (p_call_genotype[:, :, 0] == -1) | (p_call_genotype[:, :, 1] == -1)\n",
    "        p_variant_missingness = np.mean(p_missing_genotype_mask, axis=1)\n",
    "        p_sample_missingness = np.mean(p_missing_genotype_mask, axis=0)\n",
    "\n",
    "        p_filtered_variants_idx = np.where(p_variant_missingness <= 0.05)[0]\n",
    "        p_filtered_samples_idx = np.where(p_sample_missingness <= 0.05)[0]\n",
    "\n",
    "        if len(p_filtered_variants_idx) == 0 or len(p_filtered_samples_idx) == 0:\n",
    "            return None\n",
    "\n",
    "        p_filtered_gt = p_call_genotype[p_filtered_variants_idx, :, :][:, p_filtered_samples_idx, :]\n",
    "        p_filtered_sample_ids = callset_partition['sample_id'].values[p_filtered_samples_idx]\n",
    "        \n",
    "        y_partition = y_labels_all_samples.loc[p_filtered_sample_ids]\n",
    "        p_encoded_genotypes = encode_diploid_allel(p_filtered_gt)\n",
    "        p_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        p_imputed_genotypes = p_imputer.fit_transform(p_encoded_genotypes.T).T\n",
    "\n",
    "        X_partition = pd.DataFrame(\n",
    "            p_imputed_genotypes.T, \n",
    "            index=p_filtered_sample_ids, \n",
    "            columns=callset_partition['variant_position'].values[p_filtered_variants_idx]\n",
    "        )\n",
    "        \n",
    "        common_samples = X_partition.index.intersection(y_partition.index)\n",
    "        X_partition = X_partition.loc[common_samples]\n",
    "        y_partition = y_partition.loc[common_samples]\n",
    "\n",
    "        if X_partition.shape[0] == 0 or len(y_partition.unique()) < 2:\n",
    "            return None\n",
    "        \n",
    "        kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        partition_accuracy_scores = []\n",
    "        partition_precision_scores = []\n",
    "        partition_recall_scores = []\n",
    "        partition_f1_scores = []\n",
    "        partition_confusion_matrices = []\n",
    "\n",
    "        for fold, (train_index, test_index) in enumerate(kf.split(X_partition, y_partition)):\n",
    "            X_train, X_test = X_partition.iloc[train_index], X_partition.iloc[test_index]\n",
    "            y_train, y_test = y_partition.iloc[train_index], y_partition.iloc[test_index]\n",
    "            model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=2)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            partition_accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "            partition_precision_scores.append(precision_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "            partition_recall_scores.append(recall_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "            partition_f1_scores.append(f1_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "            partition_confusion_matrices.append(confusion_matrix(y_test, y_pred, labels=model.classes_))\n",
    "\n",
    "        partition_results = {\n",
    "            'accuracy_mean': np.mean(partition_accuracy_scores),\n",
    "            'accuracy_std': np.std(partition_accuracy_scores),\n",
    "            'precision_mean': np.mean(partition_precision_scores),\n",
    "            'precision_std': np.std(partition_precision_scores),\n",
    "            'recall_mean': np.mean(partition_recall_scores),\n",
    "            'recall_std': np.std(partition_recall_scores),\n",
    "            'f1_mean': np.mean(partition_f1_scores),\n",
    "            'f1_std': np.std(partition_f1_scores),\n",
    "            'classes': model.classes_.tolist(),\n",
    "            'aggregated_confusion_matrix': np.sum(partition_confusion_matrices, axis=0).tolist()\n",
    "        }\n",
    "        \n",
    "        joblib.dump(model, model_filename)\n",
    "        with open(results_filename, 'w') as f:\n",
    "            json.dump(partition_results, f, indent=4)\n",
    "        print(f\"  ✅ Model and results for partition {partition_id} saved.\")\n",
    "        return partition_id, partition_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ An error occurred while processing partition {partition_id}: {e}. Skipping this partition.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b24a7-0bc3-413b-b9cf-e7f0945d681c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 4: Parallel Processing of Partitions ---\n",
      "  ✅ Partition 2R_1-1000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_2000001-3000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_1000001-2000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_3000001-4000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_4000001-5000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_5000001-6000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_6000001-7000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_7000001-8000000 already processed. Loading results.\n",
      "\n",
      "  🚀 Training for Partition: 2R_9000001-10000000 ---\n",
      "\n",
      "  🚀 Training for Partition: 2R_10000001-11000000 ---\n",
      "  ✅ Partition 2R_8000001-9000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_11000001-12000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_12000001-13000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_13000001-14000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_14000001-15000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_15000001-16000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_16000001-17000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_17000001-18000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_18000001-19000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_19000001-20000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_20000001-21000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_21000001-22000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_22000001-23000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_23000001-24000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_24000001-25000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_25000001-26000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_26000001-27000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_27000001-28000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_28000001-29000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_29000001-30000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_30000001-31000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_31000001-32000000 already processed. Loading results.\n",
      "\n",
      "  🚀 Training for Partition: 2R_32000001-33000000 ---\n",
      "  ✅ Partition 2R_33000001-34000000 already processed. Loading results.\n",
      "\n",
      "  🚀 Training for Partition: 2R_34000001-35000000 ---\n",
      "  ✅ Partition 2R_35000001-36000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_36000001-37000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_37000001-38000000 already processed. Loading results.\n",
      "  ✅ Partition 2R_38000001-39000000 already processed. Loading results.\n",
      "\n",
      "  🚀 Training for Partition: 2R_39000001-40000000 ---\n",
      "  ❌ An error occurred while processing partition 2R_10000001-11000000: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (5,) + inhomogeneous part.. Skipping this partition.\n",
      "\n",
      "  🚀 Training for Partition: 2R_40000001-41000000 ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 4: Parallel Processing of Partitions ---\")\n",
    "all_partition_results = {}\n",
    "overall_confusion_matrices = []\n",
    "max_workers = 4\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    for result in executor.map(process_partition, all_partitions_to_process):\n",
    "        if result is not None:\n",
    "            partition_id, partition_results = result\n",
    "            all_partition_results[partition_id] = partition_results\n",
    "            overall_confusion_matrices.append(np.array(partition_results['aggregated_confusion_matrix']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87c9f69-2607-42bf-b9a6-fd02b48cedee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not all_partition_results:\n",
    "    print(\"No partitions were successfully processed and trained.\")\n",
    "else:\n",
    "    results_df = pd.DataFrame.from_dict(all_partition_results, orient='index')\n",
    "    print(\"\\nSummary of results per partition:\")\n",
    "    print(results_df[['accuracy_mean', 'f1_mean', 'precision_mean', 'recall_mean']].round(4))\n",
    "\n",
    "    overall_accuracy_mean = results_df['accuracy_mean'].mean()\n",
    "    overall_precision_mean = results_df['precision_mean'].mean()\n",
    "    overall_recall_mean = results_df['recall_mean'].mean()\n",
    "    overall_f1_mean = results_df['f1_mean'].mean()\n",
    "\n",
    "    print(f\"\\nOverall Average Accuracy across all processed partitions: {overall_accuracy_mean:.4f}\")\n",
    "    print(f\"Overall Average Precision across all processed partitions: {overall_precision_mean:.4f}\")\n",
    "    print(f\"Overall Average Recall across all processed partitions: {overall_recall_mean:.4f}\")\n",
    "    print(f\"Overall Average F1-Score across all processed partitions: {overall_f1_mean:.4f}\")\n",
    "\n",
    "    if overall_confusion_matrices:\n",
    "        overall_avg_conf_matrix = np.sum(overall_confusion_matrices, axis=0)\n",
    "        sample_classes = results_df['classes'].iloc[0]\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(\n",
    "            overall_avg_conf_matrix,\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            cmap='Blues',\n",
    "            xticklabels=sample_classes,\n",
    "            yticklabels=sample_classes\n",
    "        )\n",
    "        plt.title('Overall Aggregated Confusion Matrix (Sum of all Partition Folds)')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.show()\n",
    "        print(\"\\nOverall Aggregated Confusion Matrix values:\")\n",
    "        print(overall_avg_conf_matrix)\n",
    "    else:\n",
    "        print(\"\\nNo overall confusion matrix to display as no partitions were processed.\")\n",
    "\n",
    "print(\"\\n--- Process Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepaksilaych-deepaksilaych-deepaksilaych",
   "language": "python",
   "name": "conda-env-deepaksilaych-deepaksilaych-deepaksilaych-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
