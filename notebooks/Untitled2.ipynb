{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a44a300c-2513-404f-9f73-f498b706121a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import malariagen_data\n",
    "import allel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import joblib # For saving/loading models\n",
    "import os \n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3428bdc3-6ecc-4b88-bbb1-9b1e35771759",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ag3_init_params = {\n",
    "    \"sample_sets\": \"3.3\",\n",
    "    \"debug\": False,\n",
    "    \"show_progress\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e5648c7-e145-4f42-9719-b6819dd3c683",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "'use strict';\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    function drop(id) {\n",
       "      const view = Bokeh.index.get_by_id(id)\n",
       "      if (view != null) {\n",
       "        view.model.document.clear()\n",
       "        Bokeh.index.delete(view)\n",
       "      }\n",
       "    }\n",
       "\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null) {\n",
       "      drop(id)\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim()\n",
       "            drop(id)\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded(error = null) {\n",
       "    const el = document.getElementById(null);\n",
       "    if (el != null) {\n",
       "      const html = (() => {\n",
       "        if (typeof root.Bokeh === \"undefined\") {\n",
       "          if (error == null) {\n",
       "            return \"BokehJS is loading ...\";\n",
       "          } else {\n",
       "            return \"BokehJS failed to load.\";\n",
       "          }\n",
       "        } else {\n",
       "          const prefix = `BokehJS ${root.Bokeh.version}`;\n",
       "          if (error == null) {\n",
       "            return `${prefix} successfully loaded.`;\n",
       "          } else {\n",
       "            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n",
       "          }\n",
       "        }\n",
       "      })();\n",
       "      el.innerHTML = html;\n",
       "\n",
       "      if (error != null) {\n",
       "        const wrapper = document.createElement(\"div\");\n",
       "        wrapper.style.overflow = \"auto\";\n",
       "        wrapper.style.height = \"5em\";\n",
       "        wrapper.style.resize = \"vertical\";\n",
       "        const content = document.createElement(\"div\");\n",
       "        content.style.fontFamily = \"monospace\";\n",
       "        content.style.whiteSpace = \"pre-wrap\";\n",
       "        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n",
       "        content.textContent = error.stack ?? error.toString();\n",
       "        wrapper.append(content);\n",
       "        el.append(wrapper);\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(() => display_loaded(error), 100);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.6.2.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      try {\n",
       "            for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "\n",
       "      } catch (error) {throw error;\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(null)).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(null);\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.6.2.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {throw error;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(null)).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ag3_main_process = malariagen_data.Ag3(**ag3_init_params)\n",
    "samples_metadata = ag3_main_process.sample_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bfbdd17-3064-40b7-ad95-7c683a282671",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gambiae_coluzzii_samples = samples_metadata.query(\"aim_species in ['gambiae', 'coluzzii']\").dropna(subset=[\"sample_id\", \"aim_species\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e4a22d3-0381-41b8-a65b-5a0c437b9036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_samples_for_training = 500\n",
    "sub_samples = gambiae_coluzzii_samples.sample(num_samples_for_training, random_state=42)\n",
    "sub_sample_ids = sub_samples['sample_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "098ff787-fa01-4104-b762-f6c2a68b7ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_labels_all_samples = sub_samples.set_index('sample_id')['aim_species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "603f68d6-77a4-40f0-a545-dcb47f184fda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_diploid_allel(gt_array):\n",
    "    \"\"\"Encodes a diploid genotype array into a dosage matrix.\"\"\"\n",
    "    gt = allel.GenotypeArray(gt_array)\n",
    "    allele_counts = gt.to_allele_counts()\n",
    "    alt_allele_dosage = allele_counts[:, :, 1]\n",
    "    dosage_matrix = np.where(gt.is_missing(), np.nan, alt_allele_dosage)\n",
    "    return dosage_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c3918ff-34bf-4c00-be3f-249e4f2dc17c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected contigs: ('2R', '2L', '3R', '3L', 'X')\n"
     ]
    }
   ],
   "source": [
    "contigs = ag3_main_process.contigs\n",
    "print(f\"Detected contigs: {contigs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47c7e39e-1a7f-43c0-942b-d05d37ce64ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "partition_window_size = 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0df5e0d-653e-49fa-96e5-4073edf0c125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_partition_results = {}\n",
    "overall_confusion_matrices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdd5f4c5-be00-41d0-9703-9a1f5f4f914a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained classifiers will be saved in: trained_classifiers\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'trained_classifiers'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Trained classifiers will be saved in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c04223a7-b985-4958-ab97-111153cc893a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_single_partition(partition_tuple, sub_sample_ids, y_labels_all_samples, ag3_init_parameters, output_directory):\n",
    "    p_contig, p_start, p_end = partition_tuple\n",
    "    partition_id = f\"{p_contig}_{p_start}-{p_end}\"\n",
    "    model_filename = os.path.join(output_directory, f\"classifier_{partition_id}.joblib\")\n",
    "\n",
    "    # --- NEW: Check if classifier already exists for this partition ---\n",
    "    if os.path.exists(model_filename):\n",
    "        return partition_id, \"SKIPPED\" # Return a special string to indicate skipping\n",
    "\n",
    "    try:\n",
    "        # --- NEW: Initialize ag3 instance within this worker process ---\n",
    "        ag3_instance = malariagen_data.Ag3(**ag3_init_parameters)\n",
    "\n",
    "        # 3.1. Load SNP data for the current partition\n",
    "        callset_partition = ag3_instance.snp_calls(\n",
    "            region=f\"{p_contig}:{p_start}-{p_end}\", \n",
    "            sample_query=f\"sample_id in {sub_sample_ids}\"\n",
    "        )\n",
    "        \n",
    "        # Check if any variants were loaded for this partition\n",
    "        if callset_partition['call_genotype'].shape[0] == 0:\n",
    "            return partition_id, None # Return None to indicate skipped partition\n",
    "\n",
    "        # 3.2. Preprocessing for the current partition\n",
    "        p_call_genotype = callset_partition['call_genotype'].values\n",
    "        p_missing_genotype_mask = (p_call_genotype[:, :, 0] == -1) | (p_call_genotype[:, :, 1] == -1)\n",
    "        p_variant_missingness = np.mean(p_missing_genotype_mask, axis=1)\n",
    "        p_sample_missingness = np.mean(p_missing_genotype_mask, axis=0)\n",
    "\n",
    "        # --- Feature Reduction: Step 1 - Stricter Missingness Filtering ---\n",
    "        p_filtered_variants_idx_missingness = np.where(p_variant_missingness <= 0.01)[0]\n",
    "        p_filtered_samples_idx = np.where(p_sample_missingness <= 0.05)[0]\n",
    "\n",
    "        p_filtered_gt_initial = p_call_genotype[p_filtered_variants_idx_missingness, :, :][:, p_filtered_samples_idx, :]\n",
    "        p_filtered_sample_ids = callset_partition['sample_id'].values[p_filtered_samples_idx]\n",
    "        p_variant_positions_initial = callset_partition['variant_position'].values[p_filtered_variants_idx_missingness]\n",
    "\n",
    "        if p_filtered_gt_initial.shape[0] == 0 or p_filtered_gt_initial.shape[1] == 0:\n",
    "            return partition_id, None\n",
    "\n",
    "        p_encoded_genotypes = encode_diploid_allel(p_filtered_gt_initial)\n",
    "        temp_snp_df = pd.DataFrame(p_encoded_genotypes.T, index=p_filtered_sample_ids, columns=p_variant_positions_initial)\n",
    "        \n",
    "        # --- Feature Reduction: Step 2 - MAF Filtering ---\n",
    "        temp_gt_allel = allel.GenotypeArray(p_filtered_gt_initial)\n",
    "        temp_allele_counts = temp_gt_allel.count_alleles()\n",
    "        temp_allele_frequencies = np.where(temp_allele_counts.sum(axis=1)[:, np.newaxis] == 0, 0.0, temp_allele_counts.to_frequencies())\n",
    "        temp_mafs = np.min(temp_allele_frequencies, axis=1)\n",
    "        maf_filtered_variants_idx = np.where(temp_mafs >= 0.01)[0]\n",
    "        temp_snp_df_maf_filtered = temp_snp_df.iloc[:, maf_filtered_variants_idx]\n",
    "\n",
    "        if temp_snp_df_maf_filtered.shape[1] == 0:\n",
    "            return partition_id, None\n",
    "\n",
    "        # --- Feature Reduction: Step 3 - Variance Filtering ---\n",
    "        temp_snp_df_maf_filtered_no_nan_cols = temp_snp_df_maf_filtered.dropna(axis=1, how='all')\n",
    "        snp_variances = temp_snp_df_maf_filtered_no_nan_cols.var(axis=0, skipna=True)\n",
    "        variance_filtered_variants_idx = snp_variances[snp_variances >= 0.01].index\n",
    "        X_partition = temp_snp_df_maf_filtered_no_nan_cols[variance_filtered_variants_idx]\n",
    "\n",
    "        if X_partition.shape[1] == 0:\n",
    "            return partition_id, None\n",
    "\n",
    "        p_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        X_partition_imputed = pd.DataFrame(\n",
    "            p_imputer.fit_transform(X_partition),\n",
    "            index=X_partition.index,\n",
    "            columns=X_partition.columns\n",
    "        )\n",
    "        \n",
    "        y_partition = y_labels_all_samples.loc[X_partition_imputed.index]\n",
    "\n",
    "        common_samples = X_partition_imputed.index.intersection(y_partition.index)\n",
    "        X_partition_final = X_partition_imputed.loc[common_samples]\n",
    "        y_partition_final = y_partition.loc[common_samples]\n",
    "\n",
    "        if X_partition_final.shape[0] == 0 or len(y_partition_final.unique()) < 2:\n",
    "            return partition_id, None\n",
    "        \n",
    "        # 3.3. K-Fold Cross-Validation for the current partition\n",
    "        kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        partition_accuracy_scores = []\n",
    "        partition_precision_scores = []\n",
    "        partition_recall_scores = []\n",
    "        partition_f1_scores = []\n",
    "        partition_confusion_matrices = []\n",
    "\n",
    "        for fold, (train_index, test_index) in enumerate(kf.split(X_partition_final, y_partition_final)):\n",
    "            X_train, X_test = X_partition_final.iloc[train_index], X_partition_final.iloc[test_index]\n",
    "            y_train, y_test = y_partition_final.iloc[train_index], y_partition_final.iloc[test_index]\n",
    "\n",
    "            model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=4) \n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            partition_accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "            partition_precision_scores.append(precision_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "            partition_recall_scores.append(recall_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "            partition_f1_scores.append(f1_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "            partition_confusion_matrices.append(confusion_matrix(y_test, y_pred, labels=model.classes_))\n",
    "\n",
    "        partition_results = {\n",
    "            'accuracy_mean': np.mean(partition_accuracy_scores),\n",
    "            'accuracy_std': np.std(partition_accuracy_scores),\n",
    "            'precision_mean': np.mean(partition_precision_scores),\n",
    "            'precision_std': np.std(partition_precision_scores),\n",
    "            'recall_mean': np.mean(partition_recall_scores),\n",
    "            'recall_std': np.std(partition_recall_scores),\n",
    "            'f1_mean': np.mean(partition_f1_scores),\n",
    "            'f1_std': np.std(partition_f1_scores),\n",
    "            'classes': model.classes_.tolist(), \n",
    "            'aggregated_confusion_matrix': np.sum(partition_confusion_matrices, axis=0).tolist()\n",
    "        }\n",
    "        \n",
    "        # 3.4. Save the trained model for this partition\n",
    "        joblib.dump(model, model_filename)\n",
    "        \n",
    "        return partition_id, partition_results # Return results for this partition\n",
    "\n",
    "    except Exception as e:\n",
    "        return partition_id, None # Return None for failed partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0216f5d3-d5c8-461d-9535-98f90829279d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 62 partitions for 2R.\n",
      "  Generated 50 partitions for 2L.\n",
      "  Generated 54 partitions for 3R.\n",
      "  Generated 42 partitions for 3L.\n",
      "  Generated 25 partitions for X.\n"
     ]
    }
   ],
   "source": [
    "all_partitions_list = []\n",
    "for contig in contigs:\n",
    "    try:\n",
    "        # Use ag3_main_process to get contig info\n",
    "        contig_callset_info = ag3_main_process.snp_calls(region=contig, sample_query=f\"sample_id in {sub_sample_ids}\")\n",
    "        if 'variant_position' not in contig_callset_info or contig_callset_info['variant_position'].shape[0] == 0:\n",
    "            print(f\"  No variants found for contig {contig}. Skipping.\")\n",
    "            continue\n",
    "        max_pos = contig_callset_info['variant_position'].values.max()\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not get max position for contig {contig}: {e}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    partitions_for_contig = []\n",
    "    start_pos = 1\n",
    "    while start_pos <= max_pos:\n",
    "        end_pos = min(start_pos + partition_window_size - 1, max_pos)\n",
    "        partitions_for_contig.append((contig, start_pos, end_pos))\n",
    "        start_pos = end_pos + 1\n",
    "    \n",
    "    if not partitions_for_contig:\n",
    "        print(f\"  No partitions generated for contig {contig}. Skipping.\")\n",
    "        continue\n",
    "    print(f\"  Generated {len(partitions_for_contig)} partitions for {contig}.\")\n",
    "    all_partitions_list.extend(partitions_for_contig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f11146-1e3e-484d-9a9e-65f0c981a04a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting parallel processing with 4 workers...\n",
      "  Classifier for partition 2R_1-1000000 already exists. Skipping training.\n",
      "  Classifier for partition 2R_1000001-2000000 already exists. Skipping training.\n",
      "  Classifier for partition 2R_2000001-3000000 already exists. Skipping training.\n",
      "  Classifier for partition 2R_3000001-4000000 already exists. Skipping training.\n",
      "  Classifier for partition 2R_4000001-5000000 already exists. Skipping training.\n",
      "  Classifier for partition 2R_5000001-6000000 already exists. Skipping training.\n",
      "  Classifier for partition 2R_6000001-7000000 already exists. Skipping training.\n",
      "  Classifier for partition 2R_7000001-8000000 already exists. Skipping training.\n",
      "  Classifier for partition 2R_8000001-9000000 already exists. Skipping training.\n"
     ]
    }
   ],
   "source": [
    "if not all_partitions_list:\n",
    "    print(\"No partitions generated for parallel processing.\")\n",
    "else:\n",
    "    # Set the number of parallel processes\n",
    "    num_parallel_processes = 4 # You can adjust this based on your CPU cores\n",
    "    print(f\"\\nStarting parallel processing with {num_parallel_processes} workers...\")\n",
    "\n",
    "    # Use ProcessPoolExecutor to run process_single_partition in parallel\n",
    "    with ProcessPoolExecutor(max_workers=num_parallel_processes) as executor:\n",
    "        # executor.map returns an iterator that yields results as they are completed\n",
    "        results_iterator = executor.map(\n",
    "            process_single_partition,\n",
    "            all_partitions_list,\n",
    "            [sub_sample_ids] * len(all_partitions_list),\n",
    "            [y_labels_all_samples] * len(all_partitions_list),\n",
    "            [ag3_init_params] * len(all_partitions_list), # Pass the ag3 initialization parameters\n",
    "            [output_dir] * len(all_partitions_list)\n",
    "        )\n",
    "\n",
    "        # Collect results and aggregate\n",
    "        all_partition_results = {} # Re-initialize here to collect results from parallel processes\n",
    "        overall_confusion_matrices = [] # Re-initialize here\n",
    "\n",
    "        for partition_id, result in results_iterator:\n",
    "            if result == \"SKIPPED\":\n",
    "                print(f\"  Classifier for partition {partition_id} already exists. Skipping training.\")\n",
    "            elif result is not None:\n",
    "                all_partition_results[partition_id] = result\n",
    "                overall_confusion_matrices.append(np.array(result['aggregated_confusion_matrix']))\n",
    "                print(f\"  Successfully processed partition: {partition_id} (Avg Accuracy: {result['accuracy_mean']:.4f})\")\n",
    "            else:\n",
    "                print(f\"  Skipped or failed to process partition: {partition_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepaksilaych-deepaksilaych-deepaksilaych",
   "language": "python",
   "name": "conda-env-deepaksilaych-deepaksilaych-deepaksilaych-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
