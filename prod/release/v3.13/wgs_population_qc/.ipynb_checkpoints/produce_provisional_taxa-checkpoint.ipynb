{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f58226c3-fb24-4b1d-8774-c83e56d47e9e",
   "metadata": {},
   "source": [
    "# Produce provisional taxa\n",
    "\n",
    "This notebook uses classifiers trained on the results of cohorts analyis to predict the taxa of samples based on their SNP genotypes and produce a TSV for each sample set in the specified release."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89eec7c-9e3d-4b79-a2f3-7841093c2455",
   "metadata": {},
   "source": [
    "- **Note:** You will probably need a high RAM machine to run this, i.e. 27 GB. \n",
    "- **Note:** This notebook will probably need to install older versions of Python packages in order to import and use the stored classifiers. These packages are usually installed in `~/local/` and should be removed afterwards in order to avoid overriding the standard environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0909b81-464a-4f8c-a87c-654a0081a763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbed9572-d8d0-4256-bd68-add2c746e244",
   "metadata": {
    "tags": []
   },
   "source": [
    "##Â Setup\n",
    "This will import various commonly used modules and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631860cc-dd59-4a79-8173-efb807a0e99d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conda_prefix:  /home/conda/global/envs/global-mgenv-5.1.0\n",
      "current_environment:  global/global-mgenv-5.1.0\n"
     ]
    }
   ],
   "source": [
    "from pyprojroot import here\n",
    "%run {here()}/DataLab_bespin_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a642bd30-19a6-473c-a077-50a195da3671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/leehart/gitRepos/vector-ops')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See git repo clone root\n",
    "here()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "225d68e2-390d-475d-9ca6-4a5e022b4528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/leehart/gitRepos/vector-ops/tracking/release/v3.13/wgs_population_qc')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See cwd\n",
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959e6089-e0e9-4d62-9bef-040d71bdd7f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22934d27-159c-4621-8680-d5dee5edc961",
   "metadata": {},
   "source": [
    "## Additional imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e6b1c76-5c02-474c-b347-4248758b2afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from datetime import datetime\n",
    "import importlib.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19740c2-99f9-4092-b067-6d22f6d5efc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "369da5a0-1eb1-4989-80b5-6e3f1d5f7950",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18b49e7d-3306-4669-8604-a7199d313f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v3.13'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the release we are working on.\n",
    "release_version = Path.cwd().parent.name\n",
    "release_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8f855a8-6398-4ad9-a294-ccacd31f68cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_clone_path = here()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "740b925e-24c6-4e52-ac03-cdca54a4fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get species-group config variables\n",
    "sg_config = read_species_group_config()\n",
    "production_bucket = sg_config['production_bucket']\n",
    "release_bucket = sg_config['release_bucket']\n",
    "contigs = sg_config['contigs']\n",
    "allsites_zip_path = sg_config['allsites_zip_path']\n",
    "major_version = sg_config['major_version']\n",
    "prov_taxa_classifier_set = sg_config['prov_taxa_classifier_set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21476d41-a098-4a03-8550-468ebca52310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the tracking directory in this repo clone\n",
    "tracking_dir_path = repo_clone_path / 'tracking'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e209029-b5ab-4edb-8e4b-c2c1d9411818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04eab3e2-ca86-4a14-b158-c6c3caced3a9",
   "metadata": {},
   "source": [
    "## Settings for classifiers and classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "863166ff-cc95-47f4-b5f1-0825ad7f7a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "contigs_to_include = contigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc78623e-d9f7-469b-98bc-f4b27de4ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this might eventually need to be set in config\n",
    "partition_size = 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd7fc76-f6ac-4fa3-ba8b-b152aff7cbec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92f5091c-347c-434c-90e2-356ce45279f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E.g. gs://vo_agam_production/v3.x/taxon_classifiers/RF_20231019\n",
    "classifier_dir_gcs_path = f'gs://{production_bucket}/v{major_version}.x/taxon_classifiers/{prov_taxa_classifier_set}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7bd43ee-382f-4ab3-85c6-6b1a35a7177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diploid_genotype_encodings_gcs_path = f'{classifier_dir_gcs_path}/diploid_genotype_encodings.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3a93f92-5b53-4ffb-8c2c-7d6709c16635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this might eventually need to be set in config\n",
    "classifier_id_template = 'RF_{contig}_{start_pos}-{stop_pos}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86a41416-9ef0-473f-88e9-468d042a9516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: to use released snp_genotypes instead, e.g. post-release, after production genotypes have been deleted, path:\n",
    "#snp_genotypes_gcs_path_template = f'gs://{release_bucket}/{release_version}/snp_genotypes/all/' + '{sample_set}'\n",
    "snp_genotypes_gcs_path_template = f'gs://{production_bucket}/v{major_version}.x/curation/' + '{sample_set}/snp_genotypes_combined.zarr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84221c46-9761-4606-9516-2e31e44d9108",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "taxon_classes_local_output_path_template = 'provisional_taxa_classes.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29baf6f1-7bf5-44fd-8d86-0ffe32604ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "provisional_taxa_output_path_template = 'provisional_taxa_stats_{sample_set}.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2ce7e04-274b-4bc4-92fe-4e2b5908ac92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted_taxa_probs_output_gcs_path_template = f'gs://{production_bucket}/tracking/release/{release_version}/wgs_population_qc/predicted_taxa_probs/predicted_taxa_probs_' + '{sample_set}.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b0b29d-b8a7-4446-94d3-08890b741ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e79b9a2-4bd8-4ffe-a45a-260c8a388db5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier_set_dir_gcs_path = f'{production_bucket}/v{major_version}.x/taxon_classifiers/{prov_taxa_classifier_set}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58e10402-6df1-42e0-8804-451b69f4a568",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vo_agam_production/v3.x/taxon_classifiers/RF_20231019/classifier_requirements.txt'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_requirements_file_gcs_path = f'{classifier_set_dir_gcs_path}/classifier_requirements.txt'\n",
    "classifier_requirements_file_gcs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7fe993-ce15-4272-911d-0f1fa7899b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b81aec8-d65e-42d6-995f-2d88a84699ef",
   "metadata": {},
   "source": [
    "## Get a GCS connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cab57b05-0c2a-4538-8eb4-b09c1ede5308",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs = init_gcs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d445d65-bbc2-4e46-ab0f-f70ee5f755ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0f7e9b4-c94b-4b35-a681-3c4b53b2eaf8",
   "metadata": {},
   "source": [
    "## Check that we're using an environment that is compatible with the classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc63f905-bf2f-4c7f-b2f5-3ba032ec687b",
   "metadata": {},
   "source": [
    "**Note:** We need to use the same version of classifier packages as those used by the stored classifiers, otherwise we will see `InconsistentVersionWarning` and errors might occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a457a9f-7e40-4d79-9a9a-30273580c979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb37389e-d0b4-4d6c-8b54-37566059ef8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with gcs.open(classifier_requirements_file_gcs_path, 'r') as fh:\n",
    "    classifier_requirements = [line.strip() for line in fh.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7cfc88-8b9d-491b-9e53-3a8d14b21c71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ae893af-e140-4ebd-b749-90eac73fdb37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The classifiers require scikit-learn==1.3.0\n",
      "- The installed version is: 1.3.0\n",
      "- Location: /home/leehart/.local/lib/python3.10/site-packages\n"
     ]
    }
   ],
   "source": [
    "for classifier_requirement in classifier_requirements:\n",
    "    \n",
    "    # Get the required package and version\n",
    "    required_package, required_version = classifier_requirement.split('==')\n",
    "    \n",
    "    print()\n",
    "    print('The classifiers require', classifier_requirement)\n",
    "    \n",
    "    # Get the installed version of the package\n",
    "    installed_version = importlib.metadata.version(required_package)\n",
    "    \n",
    "    print('- The installed version is:', installed_version)\n",
    "    \n",
    "    if installed_version != required_version:\n",
    "        print('- Installing', classifier_requirement)\n",
    "        %pip install -q {classifier_requirement}\n",
    "    \n",
    "    # Ensure that the installed version matches the classifier's requirement\n",
    "    assert installed_version == required_version, f'- but version {installed_version} is installed'\n",
    "    \n",
    "    # Get the location of the package\n",
    "    distribution = importlib.metadata.distribution(required_package)\n",
    "    package_location = distribution.locate_file('')\n",
    "    \n",
    "    print('- Location:', package_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a046c83a-6d86-4d73-83e8-e8daa86fcb6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49aecc9e-9e11-49ad-b94b-7d6df7830686",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5eecf0-68d2-44dc-a2b9-60edde8e3889",
   "metadata": {},
   "source": [
    "### Functions for samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e3fcd52-0873-41a1-be40-53641845b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache(maxsize=None)\n",
    "def get_samples_sets_tuple(*, release_str):\n",
    "    # Get all of the sample sets for this release from the release sample sets config\n",
    "    rss_config = read_release_config(release=release_str)\n",
    "    return tuple(rss_config['sample_sets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b425570-db37-4d96-a65d-aa7898a3fee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_derived_samples_df(*, release_strings_tuple):\n",
    "    # Return a DataFrame containing all of the derived sample ids for the specified release strings.\n",
    "    # Include the corresponding release string and sample set for each sample id.\n",
    "    \n",
    "    # Collect the DataFrames for each sample set\n",
    "    sample_set_dfs = []\n",
    "    \n",
    "    for release_str in release_strings_tuple:\n",
    "        \n",
    "        sample_sets = get_samples_sets_tuple(release_str=release_str)\n",
    "        \n",
    "        for sample_set in sample_sets:\n",
    "            \n",
    "            # Get a DataFrame containing all of the the derived sample ids for this sample_set\n",
    "            sample_set_derived_samples_df = read_wgs_derived_samples(sample_set=sample_set)\n",
    "            \n",
    "            # Rename the derived_sample_id column to sample_id\n",
    "            sample_set_derived_samples_df.rename(columns={'derived_sample_id': 'sample_id'}, inplace=True)\n",
    "            \n",
    "            # Set the sample_id as the index\n",
    "            sample_set_derived_samples_df.set_index('sample_id', inplace=True)\n",
    "            \n",
    "            # Add the release_str as the first index\n",
    "            sample_set_derived_samples_df['release_str'] = release_str\n",
    "            sample_set_derived_samples_df.set_index('release_str', append=True, inplace=True)\n",
    "            sample_set_derived_samples_df = sample_set_derived_samples_df.swaplevel('sample_id', 'release_str', axis=0)\n",
    "            \n",
    "            # Add the sample_set as the second index\n",
    "            sample_set_derived_samples_df['sample_set'] = sample_set\n",
    "            sample_set_derived_samples_df.set_index('sample_set', append=True, inplace=True)\n",
    "            sample_set_derived_samples_df = sample_set_derived_samples_df.swaplevel('sample_id', 'sample_set', axis=0)\n",
    "            \n",
    "            #Â We're not interested in the other columns\n",
    "            # Note: we use [[]] to get a DataFrame rather than a Series, which maintains expectations.\n",
    "            # Note: this will keep the multi-index (release_str, sample_set, sample_id)\n",
    "            sample_set_dfs.append(sample_set_derived_samples_df[[]])\n",
    "    \n",
    "    return pd.concat(sample_set_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81721ee-18d0-432c-93ea-929285453637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95870db4-1f25-4668-89dd-05dca886a3b9",
   "metadata": {},
   "source": [
    "### Functions for genotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0789cb56-f78e-43ab-ba9a-a32d3e4e6b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contig_max_pos(*, genomic_positions_zarr, contig):\n",
    "    # Return the maximum position for the specified contig\n",
    "    return max(genomic_positions_zarr[contig][\"variants\"][\"POS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf4b6621-eb60-43b4-b9af-0d90bffe4792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contig_partitions(*, genomic_positions_zarr, contig, partition_size):\n",
    "    # Return a list of partition_tuple (contig, start_pos, stop_pos) where pos are inclusive\n",
    "    \n",
    "    # Get the maximum position for this contig\n",
    "    contig_max_pos = get_contig_max_pos(genomic_positions_zarr=genomic_positions_zarr, contig=contig)\n",
    "    \n",
    "    # Start with the partition (0, size - 1)\n",
    "    partition_start_pos = 0\n",
    "    partition_end_pos = partition_size - 1\n",
    "    \n",
    "    # Collect the partitions for this contig\n",
    "    partitions = []\n",
    "    \n",
    "    # While the partition's end position is less than the maximim position\n",
    "    while partition_end_pos <= contig_max_pos:\n",
    "        \n",
    "        # Add this partition\n",
    "        partitions.append((partition_start_pos, partition_end_pos))\n",
    "        \n",
    "        # Start the next partition just after the end of this partition\n",
    "        partition_start_pos = partition_end_pos + 1\n",
    "        \n",
    "        # End the next partition at the end of the next partition (!)\n",
    "        partition_end_pos += partition_size\n",
    "\n",
    "    # If there are still positions remaining after the start_pos\n",
    "    if partition_start_pos < contig_max_pos:\n",
    "        # Add the last partition\n",
    "        partitions.append((partition_start_pos, contig_max_pos))\n",
    "    \n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1527af7-a446-4f7a-b4c7-569c65acbeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache(maxsize=None)\n",
    "def get_snp_genotypes_zarr(*, gcs, snp_genotypes_gcs_path_template, release_str, sample_set):\n",
    "    # Return the snp_genotypes zarr for the specified sample set.\n",
    "    # Raise an exception if the relevant path is not found.\n",
    "    \n",
    "    # Note: in some contexts, the release_str placeholder will not be in the path template,\n",
    "    #       in which case, only the sample_set placeholder will be replaced.\n",
    "    snp_genotypes_gcs_path = snp_genotypes_gcs_path_template.format(\n",
    "        release_str=release_str, sample_set=sample_set\n",
    "    )\n",
    "\n",
    "    # Check this path exists. This sample set might have had 0 samples pass QC.\n",
    "    if not gcs.isdir(snp_genotypes_gcs_path):\n",
    "        raise Exception(f'ERROR from get_snp_genotypes_zarr(): snp_genotypes_gcs_path not found {snp_genotypes_gcs_path}')\n",
    "\n",
    "    return open_gcs_zarr(gcs_url=snp_genotypes_gcs_path, gcs=gcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "795742a2-c633-4288-a354-08e2ad28459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diploid_genotypes_na(*, genomic_positions_zarr, gcs, snp_genotypes_gcs_path_template, release_str, sample_set, partition_tuple):\n",
    "    # Return the diploid genotypes (GT array) for all the samples in the specified sample set and partition tuple.\n",
    "    # Return a DataFrame of the sample ids aligned with the retrieved genotypes.\n",
    "    # Raise an exception if the relevant snp_genotypes were not found.\n",
    "    \n",
    "    # Extract the contig, start and stop position from the partition_tuple\n",
    "    (contig, start_pos, stop_pos) = partition_tuple\n",
    "    \n",
    "    # Get the snp_genotypes Zarr for this sample set.\n",
    "    # Note: this function should raise an exception if there are no genotypes found for the sample set.\n",
    "    snp_genotypes_zarr = get_snp_genotypes_zarr(\n",
    "        gcs=gcs,\n",
    "        snp_genotypes_gcs_path_template=snp_genotypes_gcs_path_template,\n",
    "        release_str=release_str,\n",
    "        sample_set=sample_set\n",
    "    )\n",
    "\n",
    "    # Handle error when snp_genotypes zarr not found, in case it was not raised above.\n",
    "    if snp_genotypes_zarr is None:\n",
    "        raise Exception(f'ERROR from get_diploid_genotypes_na(): snp_genotypes_zarr was None for sample_set {sample_set}')\n",
    "    \n",
    "    # Get the list of sample ids, which correspond to the genotypes for this sample_set\n",
    "    aligned_sample_ids = snp_genotypes_zarr['samples'][:]\n",
    "    \n",
    "    # Note: the sample ids are stored as byte strings when released, which we can decode\n",
    "    #   but while in production, they are stored as normal strings, which will raise an error if we try to decode    \n",
    "    if any(isinstance(element, (bytes, bytearray)) for element in aligned_sample_ids):\n",
    "        aligned_sample_ids = np.char.decode(aligned_sample_ids, 'utf-8')\n",
    "    \n",
    "    # Get the contig genotypes Zarr array for the specified contig\n",
    "    contig_genotypes_za = snp_genotypes_zarr[contig]['calldata/GT']\n",
    "\n",
    "    # Get the genomic positions for the specified contig\n",
    "    genomic_positions = allel.SortedIndex(genomic_positions_zarr[contig]['variants/POS'])\n",
    "\n",
    "    # Get the positions slice for the specified range\n",
    "    pos_slice = genomic_positions.locate_range(start_pos, stop_pos)\n",
    "\n",
    "    # Get the computed diploid genotypes as a Numpy array for the specified slice of positions\n",
    "    # TODO: Can probably eliminate the use of Dask here?\n",
    "    diploid_genotypes_na = da.from_zarr(contig_genotypes_za)[pos_slice].compute()\n",
    "    \n",
    "    # Check that the number of aligned samples matches the corresponding dimension in the Dask array\n",
    "    assert diploid_genotypes_na.shape[1] == len(aligned_sample_ids)\n",
    "        \n",
    "    return diploid_genotypes_na, aligned_sample_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b2bb4eb-f640-4c88-ba10-d97c12ea847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diploid_genotype_encodings(*, gcs, diploid_genotype_encodings_gcs_path):\n",
    "    \n",
    "    # Load data from YAML file\n",
    "    with gcs.open(diploid_genotype_encodings_gcs_path, 'r') as yaml_file:\n",
    "        diploid_genotypes_as_str_encodings = yaml.safe_load(yaml_file)\n",
    "        \n",
    "    # Convert string keys back to tuples\n",
    "    # Convert int() values back to np.uint8()\n",
    "    diploid_genotype_encodings = {\n",
    "        tuple(eval(key)): np.uint8(value) for key, value in diploid_genotypes_as_str_encodings.items()\n",
    "    }\n",
    "    \n",
    "    return diploid_genotype_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a8ef8a8-8220-4683-b241-67183bf2936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diploid_genotype_encoder(diploid_genotype_encodings, first_allele_arr, second_allele_arr) -> np.ndarray:\n",
    "    # Return array containing the encoded values for the two given parallel diploid genotype arrays \n",
    "    return np.vectorize(lambda a, b: diploid_genotype_encodings[(a, b)])(first_allele_arr, second_allele_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "daf3643e-e501-43e4-ac93-401d3d6449bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_diploid_genotypes_na(*, diploid_genotype_encodings, diploid_genotypes_na):\n",
    "    # Return the encoded genotypes (uint8) for the given diploid genotypes (GT array)\n",
    "    return diploid_genotype_encoder(diploid_genotype_encodings, diploid_genotypes_na[:, :, 0], diploid_genotypes_na[:, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ad6095c-c7c1-4565-8244-1ef47e185320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoded_genotypes_na(\n",
    "    *, diploid_genotype_encodings, genomic_positions_zarr, gcs, snp_genotypes_gcs_path_template,\n",
    "    samples_df, partition_tuple, warn_missing_genotypes=True\n",
    "):\n",
    "    # Return the encoded genotypes (uint8) for the specified samples and partition.\n",
    "    # Return a DataFrame of the sample ids matching the retrieved genotypes.\n",
    "    # Optionally warn if a target sample id was not found in the set of available genotypes.\n",
    "    \n",
    "    # Get DataFrames grouped by release_str\n",
    "    release_str_dfgb = samples_df.groupby('release_str')\n",
    "    \n",
    "    # Collect the encoded genotype arrays for each release (as a list, so we can concatenate them later)\n",
    "    encoded_genotype_narrs_per_release = []\n",
    "    \n",
    "    # Collect the sample ids as a list of dictionaries {'release_str': foo, 'sample_set': bar, 'sample_id': baz}\n",
    "    # This should be in the same order as the encoded genotypes.\n",
    "    got_sample_id_dicts_for_all_releases = []\n",
    "    \n",
    "    # For each release, get the diploid genotype Dask arrays\n",
    "    for release_str, release_str_samples_df in release_str_dfgb:\n",
    "        \n",
    "        # Get the unique list of sample_set values for this release.\n",
    "        #Â Warning: this will not preserve the same order of the sample sets, but we won't rely on that.\n",
    "        sample_sets = release_str_samples_df.index.get_level_values('sample_set').unique().tolist()\n",
    "        \n",
    "        # Collect the encoded genotype arrays for each sample set (as a list, so we can concatenate them later)\n",
    "        encoded_genotype_narrs_per_sample_set = []\n",
    "        \n",
    "        for sample_set in sample_sets:\n",
    "            \n",
    "            # Extract the contig, start and stop position from the partition_tuple\n",
    "            (contig, start_pos, stop_pos) = partition_tuple\n",
    "            \n",
    "            # Get the diploid genotypes, which should also provide the aligned sample ids\n",
    "            diploid_genotypes_na, aligned_sample_ids = get_diploid_genotypes_na(\n",
    "                genomic_positions_zarr=genomic_positions_zarr,\n",
    "                gcs=gcs,\n",
    "                snp_genotypes_gcs_path_template=snp_genotypes_gcs_path_template,\n",
    "                release_str=release_str,\n",
    "                sample_set=sample_set,\n",
    "                partition_tuple=partition_tuple\n",
    "            )\n",
    "            \n",
    "            # Encode the diploid genotypes\n",
    "            encoded_genotypes_na = encode_diploid_genotypes_na(\n",
    "                diploid_genotype_encodings=diploid_genotype_encodings,\n",
    "                diploid_genotypes_na=diploid_genotypes_na\n",
    "            )\n",
    "            \n",
    "            # Get the target samples for this sample_set\n",
    "            # This should preserve the order of the sample ids.\n",
    "            sample_set_df = release_str_samples_df.xs(sample_set, level='sample_set')\n",
    "            target_sample_ids = sample_set_df.index.get_level_values('sample_id').tolist()\n",
    "            \n",
    "            # Warn if a target sample id was not found in the set of available genotypes.\n",
    "            # Note: This might produce a lot of output when using the list of derived samples\n",
    "            #       because unsequenced samples have not been filtered out.\n",
    "            if warn_missing_genotypes:\n",
    "                for target_sample_id in target_sample_ids:\n",
    "                    if target_sample_id not in aligned_sample_ids:\n",
    "                        print('WARNING get_encoded_genotypes_na(): target_sample_id not found', release_str, sample_set, target_sample_id)\n",
    "\n",
    "            # Create a boolean mask to select the target samples from those available\n",
    "            target_sample_selection_mask = [sample_id in target_sample_ids for sample_id in aligned_sample_ids]\n",
    "            \n",
    "            # Get the encoded genotypes for the specified samples\n",
    "            sample_selection_encoded_genotypes_na = encoded_genotypes_na[:, target_sample_selection_mask]\n",
    "            \n",
    "            # Add the encoded genotypes for this sample_set to the list\n",
    "            encoded_genotype_narrs_per_sample_set.append(sample_selection_encoded_genotypes_na)\n",
    "            \n",
    "            # Get the list of obtained sample ids, which can differ from the target and those available\n",
    "            # This should still preserve the order of the sample_ids.\n",
    "            got_sample_ids = aligned_sample_ids[target_sample_selection_mask]\n",
    "            \n",
    "            # Get the list of sample id dictionaries for this sample set.\n",
    "            # This should preserve the order of the sample ids.\n",
    "            got_sample_id_dicts = [{'release_str': release_str, 'sample_set': sample_set, 'sample_id': got_sample_id} for got_sample_id in got_sample_ids]\n",
    "            \n",
    "            # Add the list of obtained sample id dicts to the list.\n",
    "            # This should preserve the order of the sample ids.\n",
    "            got_sample_id_dicts_for_all_releases.extend(got_sample_id_dicts)\n",
    "        \n",
    "        \n",
    "        # Concatenate the arrays of encoded genotype for all the sample sets in this release.\n",
    "        # We concatenate along the samples dimension (axis=1).\n",
    "        # The number of genotypes should be the same length for all arrays for all sample sets.\n",
    "        encoded_genotypes_na_for_all_sample_sets = np.concatenate(encoded_genotype_narrs_per_sample_set, axis=1)\n",
    "        \n",
    "        # Add the encoded genotypes for this release to the list\n",
    "        encoded_genotype_narrs_per_release.append(encoded_genotypes_na_for_all_sample_sets)\n",
    "    \n",
    "    \n",
    "    # Concatenate the arrays of encoded genotypes for all the specified releases\n",
    "    # We concatenate along the samples dimension (axis=1).\n",
    "    # The number of genotypes should be the same length for all arrays for all releases.\n",
    "    encoded_genotypes_for_all_releases_na = np.concatenate(encoded_genotype_narrs_per_release, axis=1)\n",
    "    \n",
    "    # Transpose the array into the shape (n_samples, n_genotypes)\n",
    "    encoded_genotypes_for_all_releases_na = encoded_genotypes_for_all_releases_na.T\n",
    "    \n",
    "    # Convert the list of sample id dictionaries to a DataFrame\n",
    "    # Note: this should retain the order of the got sample_ids so they remain algned with the genotypes.\n",
    "    aligned_samples_df = pd.DataFrame(got_sample_id_dicts_for_all_releases)\n",
    "    aligned_samples_df.set_index(['release_str', 'sample_set', 'sample_id'], inplace=True)\n",
    "    \n",
    "    return encoded_genotypes_for_all_releases_na, aligned_samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbb15dd-510a-4a21-b46c-6728a0449e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00599135-2a9e-46be-b367-8ef56df53078",
   "metadata": {},
   "source": [
    "### Functions for classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02279799-ed84-4c27-96b7-ba22df0b3faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache(maxsize=None)\n",
    "def get_classifier_id(*, classifier_id_template, partition_tuple):\n",
    "    # Return the classifier_id based on the given partition_tuple\n",
    "    (contig, start_pos, stop_pos) = partition_tuple\n",
    "    return classifier_id_template.format(contig=contig, start_pos=start_pos, stop_pos=stop_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5cc07627-ac64-46c2-aad2-0a2d9aaee668",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache(maxsize=None)\n",
    "def get_classifier_joblib_gcs_path(*, classifier_dir_gcs_path, classifier_id_template, partition_tuple):\n",
    "    \n",
    "    classifier_id = get_classifier_id(classifier_id_template=classifier_id_template, partition_tuple=partition_tuple)\n",
    "    \n",
    "    (contig, start_pos, stop_pos) = partition_tuple\n",
    "    \n",
    "    return classifier_dir_gcs_path + f'/{contig}/{classifier_id}.joblib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a407d6f4-e632-41da-b32c-6f22e7cc7d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: Don't cache this function!\n",
    "def classifier_exists(*, classifier_dir_gcs_path, classifier_id_template, partition_tuple):\n",
    "    # If the classifier for the specified partition_tuple exists, return True.\n",
    "    # Else return False.\n",
    "    \n",
    "    # Get the path to the classifier\n",
    "    classifier_file_gcs_path = get_classifier_joblib_gcs_path(\n",
    "        classifier_dir_gcs_path=classifier_dir_gcs_path,\n",
    "        classifier_id_template=classifier_id_template,\n",
    "        partition_tuple=partition_tuple\n",
    "    )\n",
    "    \n",
    "    return gcs.exists(classifier_file_gcs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80fc9a05-f442-4932-ae5f-1f5343563ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_classifier(*, classifier_dir_gcs_path, classifier_id_template, partition_tuple):\n",
    "    # Return the classifier for the specified partition_tuple\n",
    "    \n",
    "    # Get the path to the classifier\n",
    "    classifier_file_gcs_path = get_classifier_joblib_gcs_path(\n",
    "        classifier_dir_gcs_path=classifier_dir_gcs_path,\n",
    "        classifier_id_template=classifier_id_template,\n",
    "        partition_tuple=partition_tuple\n",
    "    )\n",
    "    \n",
    "    # Open a file-handle to the GCS output path using read-binary mode\n",
    "    with gcs.open(classifier_file_gcs_path, 'rb') as fh:\n",
    "        \n",
    "        # Use joblib to load the classifier from the file-handle\n",
    "        classifier = joblib.load(fh)\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "509ed093-b330-4e75-87c2-70e7c31f1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_probs_via_classifier(*, classifier, genotypes_arr):\n",
    "    # Return the predicted class probabilities for the given classifier.\n",
    "    return classifier.predict_proba(genotypes_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b6a409-f8b3-4374-812b-478f878436fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64696368-c7cd-4f99-a94b-ea27c8d01f3d",
   "metadata": {},
   "source": [
    "## Functions for probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02ee6921-9c81-4748-91f0-6a404d6449ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxon_by_prob_column_name_dict(*, taxon_classes):\n",
    "    # Map the names of the taxon probability columns with corresponding taxon classes\n",
    "    taxon_by_prob_column_name_dict = {\n",
    "        f'{taxon}_prob': taxon for taxon in taxon_classes\n",
    "    }\n",
    "    return taxon_by_prob_column_name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eadb4b35-b643-475f-873a-50cda87ce018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_taxon_from_df_row(df_row, taxon_classes):\n",
    "    \n",
    "    # Get the dictionary mapping taxon prob columns to class labels\n",
    "    taxon_by_prob_column_name_dict = get_taxon_by_prob_column_name_dict(taxon_classes=taxon_classes)\n",
    "    \n",
    "    # Get the list of taxon probability column names\n",
    "    taxon_prob_column_names = list(taxon_by_prob_column_name_dict.keys())\n",
    "    \n",
    "    # Get the name of the prob column with the highest value\n",
    "    max_taxon_column_name = df_row[taxon_prob_column_names].idxmax()\n",
    "    \n",
    "    # Get the taxon represented by that column\n",
    "    max_taxon = taxon_by_prob_column_name_dict[max_taxon_column_name]\n",
    "    \n",
    "    return max_taxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18a529d1-7290-4a40-a382-82e4a42d87ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_taxon_prob_diff_from_df_row(df_row, taxon_classes):\n",
    "    \n",
    "    # Get the dictionary mapping taxon prob columns to class labels\n",
    "    taxon_by_prob_column_name_dict = get_taxon_by_prob_column_name_dict(taxon_classes=taxon_classes)\n",
    "    \n",
    "    # Get the list of taxon probability columns\n",
    "    taxon_prob_columns = list(taxon_by_prob_column_name_dict.keys())\n",
    "    \n",
    "    # Get the values from the taxon probability columns\n",
    "    taxon_probs = df_row[taxon_prob_columns]\n",
    "\n",
    "    # Get the probability values in descending order\n",
    "    taxon_probs_descending = sorted(taxon_probs, reverse=True)\n",
    "\n",
    "    # Get the difference between the two highest values\n",
    "    conf_by_prob_diff = taxon_probs_descending[0] - taxon_probs_descending[1]\n",
    "    \n",
    "    return conf_by_prob_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0fe9777b-5f4e-4fd8-b2ea-0c582c81bb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PL_from_prob(prob, epsilon=1e-10):\n",
    "    \n",
    "    # Method: https://gatk.broadinstitute.org/hc/en-us/articles/360035890451-Calculation-of-PL-and-GQ-by-HaplotypeCaller-and-GenotypeGVCFs\n",
    "    \n",
    "    # PL = Phred-scaled likelihood\n",
    "    \n",
    "    # \"low PL values mean [the thing] is more likely, and high PL values means itâs less likely\"\n",
    "    \n",
    "    # Using a very small value (epsilon) to avoid taking the logarithm of zero.\n",
    "    \n",
    "    raw_PL_from_prob = -10 * np.log10(prob + epsilon)\n",
    "    \n",
    "    return raw_PL_from_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b9f104b-c631-4bc6-8ba5-fc10e8e465dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_taxon_prob_qual_from_df_row(df_row, taxon_classes, cap=99):\n",
    "    \n",
    "    # Method: https://gatk.broadinstitute.org/hc/en-us/articles/360035890451-Calculation-of-PL-and-GQ-by-HaplotypeCaller-and-GenotypeGVCFs\n",
    "    \n",
    "    # \"Quality\" calculated in a similar way to GQ (genotype quality)\n",
    "    \n",
    "    # Get the dictionary mapping taxon prob columns to class labels\n",
    "    taxon_by_prob_column_name_dict = get_taxon_by_prob_column_name_dict(taxon_classes=taxon_classes)\n",
    "    \n",
    "    # Get the list of taxon probability columns\n",
    "    taxon_prob_columns = list(taxon_by_prob_column_name_dict.keys())\n",
    "    \n",
    "    # Get the values from the taxon probability columns\n",
    "    taxon_probs = df_row[taxon_prob_columns]\n",
    "    \n",
    "    # Get the \"raw PL\" (Phred-scaled likelihood) for each taxon probability\n",
    "    taxon_raw_PLs = [get_PL_from_prob(taxon_prob) for taxon_prob in taxon_probs]\n",
    "    \n",
    "    # Find the lowest value in the taxon_raw_PL list\n",
    "    min_taxon_raw_PL = min(taxon_raw_PLs)\n",
    "    \n",
    "    # Subtract the lowest value from each value in the taxon_raw_PL list\n",
    "    normalized_taxon_PLs = [taxon_raw_PL - min_taxon_raw_PL for taxon_raw_PL in taxon_raw_PLs]\n",
    "    \n",
    "    # Sort the normalized PL values in ascending order\n",
    "    sorted_normalized_taxon_PLs = sorted(normalized_taxon_PLs)\n",
    "    \n",
    "    # Get the lowest and second lowest normalized PL values\n",
    "    lowest_normalized_taxon_PL = sorted_normalized_taxon_PLs[0]\n",
    "    second_lowest_normalized_taxon_PL = sorted_normalized_taxon_PLs[1]\n",
    "    \n",
    "    # Get the difference between the lowest and second lowest normalized PL values\n",
    "    conf_by_prob_qual = second_lowest_normalized_taxon_PL - lowest_normalized_taxon_PL\n",
    "    \n",
    "    # Cap the value \"for practical reasons\"\n",
    "    if cap is not None:\n",
    "        conf_by_prob_qual = min(conf_by_prob_qual, cap)\n",
    "    \n",
    "    return conf_by_prob_qual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0a314444-8ac8-4853-9ca9-7801d2257b3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_samples_taxon_prob_averages_df(*, taxon_probs_by_partition_df, taxon_classes):\n",
    "    # Return a DataFrame containing the average value for each taxon probability column across all partitions.\n",
    "    \n",
    "    # Get the dictionary mapping taxon prob columns to class labels\n",
    "    taxon_by_prob_column_name_dict = get_taxon_by_prob_column_name_dict(taxon_classes=taxon_classes)\n",
    "    \n",
    "    # Get the list of taxon probability columns\n",
    "    taxon_prob_columns = list(taxon_by_prob_column_name_dict.keys())\n",
    "    \n",
    "    # Compose the aggregation dictionary, specifying the aggregation function for each column\n",
    "    taxon_prob_column_agg_dict = {col: 'mean' for col in taxon_probs_by_partition_df.columns if col in taxon_prob_columns}\n",
    "    \n",
    "    # Group by 'sample_id' and aggregate by taking the mean of each taxon probability\n",
    "    sample_taxon_prob_averages_df = taxon_probs_by_partition_df.groupby(\n",
    "        level=['release_str', 'sample_set', 'sample_id']\n",
    "    ).agg(taxon_prob_column_agg_dict)\n",
    "\n",
    "    # Rename the columns to indicate that they represent mean probabilities\n",
    "    sample_taxon_prob_averages_df.columns = [f'{col}_mean' for col in sample_taxon_prob_averages_df.columns]\n",
    "    \n",
    "    return sample_taxon_prob_averages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17b03c20-3d94-424c-af10-b0077a4c1ee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_samples_max_taxon_counts_df(*, taxon_probs_by_partition_df, taxon_classes):\n",
    "    # Return a DataFrame containing the max_taxon counts for each max_taxon value across all partitions.\n",
    "    \n",
    "    # Get the counts of each max_taxon value per sample as a Pandas Series using size() and groupby() \n",
    "    samples_max_taxon_counts_srs = taxon_probs_by_partition_df.groupby(\n",
    "        ['release_str', 'sample_set', 'sample_id', 'max_taxon']\n",
    "    ).size()\n",
    "    \n",
    "    # Convert the unique max_taxon values to columns and fill missing counts with 0.\n",
    "    samples_max_taxon_counts_df = samples_max_taxon_counts_srs.unstack(fill_value=0)\n",
    "    \n",
    "    # Include counts for taxon classes that did not appear in the data by reindexing and filling with 0.\n",
    "    samples_max_taxon_counts_df = samples_max_taxon_counts_df.reindex(columns=taxon_classes, fill_value=0)\n",
    "    \n",
    "    # Rename the columns to include the suffix \"_votes\"\n",
    "    samples_max_taxon_counts_df.columns.name = None\n",
    "    samples_max_taxon_counts_df.columns = [f'{taxon}_votes' for taxon in taxon_classes]\n",
    "    \n",
    "    return samples_max_taxon_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c88cb045-cf08-4d87-a100-c0c9c28f6755",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_taxon_by_prob_mean_column_name_dict(*, taxon_classes):\n",
    "    # Map the names of the taxon probability average columns with corresponding taxon classes\n",
    "    taxon_by_prob_mean_column_name_dict = {\n",
    "        f'{taxon}_prob_mean': taxon for taxon in taxon_classes\n",
    "    }\n",
    "    return taxon_by_prob_mean_column_name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b2973185-fb28-4a5b-b5f3-d02f1d847837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_max_mean_prob_taxon_from_df_row(df_row, taxon_classes):\n",
    "\n",
    "    # Get the dictionary mapping taxon prob mean columns to class labels\n",
    "    taxon_by_prob_mean_column_name_dict = get_taxon_by_prob_mean_column_name_dict(taxon_classes=taxon_classes)\n",
    "    \n",
    "    # Get the list of taxon probability average columns\n",
    "    taxon_prob_mean_column_names = list(taxon_by_prob_mean_column_name_dict.keys())\n",
    "    \n",
    "    # Get the name of the mean prob column with the highest value\n",
    "    max_mean_prob_taxon_column_name = df_row[taxon_prob_mean_column_names].idxmax()\n",
    "    \n",
    "    # Get the taxon represented by that max column\n",
    "    max_mean_prob_taxon = taxon_by_prob_mean_column_name_dict[max_mean_prob_taxon_column_name]\n",
    "    \n",
    "    return max_mean_prob_taxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0d70af84-3207-4e25-b4f0-2ccc3e3df040",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_max_mean_prob_taxon_diff_from_df_row(df_row, taxon_classes):\n",
    "    \n",
    "    # TODO: I expect we could merge this with get_max_taxon_prob_diff_from_df_row() \n",
    "    \n",
    "    # Get the dictionary mapping taxon prob mean columns to class labels\n",
    "    taxon_by_prob_mean_column_name_dict = get_taxon_by_prob_mean_column_name_dict(taxon_classes=taxon_classes)\n",
    "    \n",
    "    # Get the list of taxon probability average columns\n",
    "    taxon_prob_mean_column_names = list(taxon_by_prob_mean_column_name_dict.keys())\n",
    "    \n",
    "    # Get the values from the taxon probability average columns\n",
    "    taxon_prob_averages = df_row[taxon_prob_mean_column_names]\n",
    "    \n",
    "    # Get the probability averages in descending order\n",
    "    taxon_probs_averages_descending = sorted(taxon_prob_averages, reverse=True)\n",
    "    \n",
    "    # Get the difference between the two highest values\n",
    "    conf_by_prob_average_diff = taxon_probs_averages_descending[0] - taxon_probs_averages_descending[1]\n",
    "    \n",
    "    return conf_by_prob_average_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d081d780-d68a-4046-ade0-43d5f4fd9581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_mean_prob_taxon_qual_from_df_row(df_row, taxon_classes, cap=99):\n",
    "    \n",
    "    # TODO: I expect we could merge this with get_max_taxon_prob_qual_from_df_row()\n",
    "    \n",
    "    \n",
    "    # Method: https://gatk.broadinstitute.org/hc/en-us/articles/360035890451-Calculation-of-PL-and-GQ-by-HaplotypeCaller-and-GenotypeGVCFs\n",
    "    \n",
    "    # \"Quality\" calculated in a similar way to GQ (genotype quality)\n",
    "    \n",
    "    # Get the dictionary mapping taxon prob mean columns to class labels\n",
    "    taxon_by_prob_mean_column_name_dict = get_taxon_by_prob_mean_column_name_dict(taxon_classes=taxon_classes)\n",
    "    \n",
    "    # Get the list of taxon probability average columns\n",
    "    taxon_prob_mean_column_names = list(taxon_by_prob_mean_column_name_dict.keys())\n",
    "    \n",
    "    # Get the values from the taxon probability average columns\n",
    "    taxon_prob_averages = df_row[taxon_prob_mean_column_names]\n",
    "    \n",
    "    # Get the \"raw PL\" (Phred-scaled likelihood) for each taxon probability average\n",
    "    taxon_raw_PLs = [get_PL_from_prob(taxon_prob) for taxon_prob in taxon_prob_averages]\n",
    "    \n",
    "    # Find the lowest value in the taxon_raw_PL list\n",
    "    min_taxon_raw_PL = min(taxon_raw_PLs)\n",
    "    \n",
    "    # Subtract the lowest value from each value in the taxon_raw_PL list\n",
    "    normalized_taxon_PLs = [taxon_raw_PL - min_taxon_raw_PL for taxon_raw_PL in taxon_raw_PLs]\n",
    "    \n",
    "    # Sort the normalized PL values in ascending order\n",
    "    sorted_normalized_taxon_PLs = sorted(normalized_taxon_PLs)\n",
    "    \n",
    "    # Get the lowest and second lowest normalized PL values\n",
    "    lowest_normalized_taxon_PL = sorted_normalized_taxon_PLs[0]\n",
    "    second_lowest_normalized_taxon_PL = sorted_normalized_taxon_PLs[1]\n",
    "    \n",
    "    # Get the difference between the lowest and second lowest normalized PL values\n",
    "    conf_by_prob_qual = second_lowest_normalized_taxon_PL - lowest_normalized_taxon_PL\n",
    "    \n",
    "    # Cap the value \"for practical reasons\"\n",
    "    if cap is not None:\n",
    "        conf_by_prob_qual = min(conf_by_prob_qual, cap)\n",
    "    \n",
    "    return conf_by_prob_qual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d0b09207-9526-4084-a313-f2f7f5a1ad70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_taxon_by_votes_column_name_dict(*, taxon_classes):\n",
    "    # Map the names of the taxon votes columns with corresponding taxon classes\n",
    "    taxon_by_votes_column_name_dict = {\n",
    "        f'{taxon}_votes': taxon for taxon in taxon_classes\n",
    "    }\n",
    "    return taxon_by_votes_column_name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c45a8b9b-16fe-460f-8a46-7da8aadd4a38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_max_votes_taxon_from_df_row(df_row, taxon_classes):\n",
    "    \n",
    "    # TODO: this looks very similar to get_max_mean_prob_taxon_from_df_row()\n",
    "\n",
    "    # Get the dictionary mapping taxon votes columns to class labels\n",
    "    taxon_by_votes_column_name_dict = get_taxon_by_votes_column_name_dict(taxon_classes=taxon_classes)\n",
    "    \n",
    "    # Get the list of taxon votes columns\n",
    "    taxon_votes_column_names = list(taxon_by_votes_column_name_dict.keys())\n",
    "    \n",
    "    # Get the name of the votes column with the highest value\n",
    "    max_votes_taxon_column_name = df_row[taxon_votes_column_names].idxmax()\n",
    "    \n",
    "    # Get the taxon represented by that max column\n",
    "    max_votes_taxon = taxon_by_votes_column_name_dict[max_votes_taxon_column_name]\n",
    "    \n",
    "    return max_votes_taxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fbaa1d40-6c91-4386-8cd6-8d8b85c68ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_max_votes_taxon_ratio_diff_from_df_row(df_row, taxon_classes, epsilon=1e-10):\n",
    "    \n",
    "    # Get the dictionary mapping taxon votes columns to class labels\n",
    "    taxon_by_votes_column_name_dict = get_taxon_by_votes_column_name_dict(taxon_classes=taxon_classes)\n",
    "    \n",
    "    # Get the list of taxon votes columns\n",
    "    taxon_votes_column_names = list(taxon_by_votes_column_name_dict.keys())\n",
    "    \n",
    "    # Get the values from the columns\n",
    "    taxon_vote_counts = df_row[taxon_votes_column_names]\n",
    "    \n",
    "    # Get the total number of votes\n",
    "    taxon_votes_total = sum(taxon_vote_counts)\n",
    "    \n",
    "    # Get the vote ratios (as decimal fractions) using list comprehension.\n",
    "    # Use a very small value (epsilon) to avoid division by zero.\n",
    "    taxon_vote_ratios = [taxon_vote_count / (taxon_votes_total + epsilon) for taxon_vote_count in taxon_vote_counts]\n",
    "    \n",
    "    # Get the vote ratios (as decimal fractions) in descending order\n",
    "    taxon_vote_ratios_descending = sorted(taxon_vote_ratios, reverse=True)\n",
    "    \n",
    "    # Get the difference between the two highest values\n",
    "    conf_by_ratio_diff = taxon_vote_ratios_descending[0] - taxon_vote_ratios_descending[1]\n",
    "    \n",
    "    return conf_by_ratio_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2767fb35-a84f-47c7-9dd8-f37d0fe5fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_probs_df(*, derived_samples_df):\n",
    "    \n",
    "    # TODO: doc\n",
    "    \n",
    "    # Check the consistency of the classifier classes\n",
    "    aligned_taxon_classes = None\n",
    "\n",
    "    # Check the consistency of the number of samples,\n",
    "    #   e.g. when we get the genotypes or classifications for each partition\n",
    "    samples_count = None\n",
    "\n",
    "    # Get the aligned_samples_df, checked for consistency with each get_encoded_genotypes_na()\n",
    "    consistent_aligned_samples_df = None\n",
    "\n",
    "    # Collect the DataFrames of predicted probabilities per partition,\n",
    "    #   i.e. (release_str, sample_set, sample_id, contig, start_pos, stop_pos, [taxon]_prob)\n",
    "    predicted_prob_dfs_per_partition = []\n",
    "\n",
    "    # For each contig in the list of contigs to include\n",
    "    for contig in contigs_to_include:\n",
    "\n",
    "        print()\n",
    "        print(contig)\n",
    "\n",
    "        contig_partitions = get_contig_partitions(\n",
    "            genomic_positions_zarr=genomic_positions_zarr,\n",
    "            contig=contig,\n",
    "            partition_size=partition_size\n",
    "        )\n",
    "\n",
    "        for start_pos, stop_pos in contig_partitions:\n",
    "\n",
    "            print('- Partition', start_pos, stop_pos)\n",
    "\n",
    "            print(' - Time', datetime.utcnow().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "            # Compose the partition tuple\n",
    "            partition_tuple = (contig, start_pos, stop_pos)\n",
    "\n",
    "            # Skip if classifier doesn't exist\n",
    "            if not classifier_exists(\n",
    "                classifier_dir_gcs_path=classifier_dir_gcs_path,\n",
    "                classifier_id_template=classifier_id_template,\n",
    "                partition_tuple=partition_tuple\n",
    "            ):\n",
    "                print(' - WARNING: Classifier not found. Skipping.')\n",
    "                continue\n",
    "\n",
    "\n",
    "            print(' - Getting classifier')\n",
    "\n",
    "            # Import the classifier from GCS\n",
    "            classifier = import_classifier(\n",
    "                classifier_dir_gcs_path=classifier_dir_gcs_path,\n",
    "                classifier_id_template=classifier_id_template,\n",
    "                partition_tuple=partition_tuple\n",
    "            )\n",
    "\n",
    "            # Check the consistency of the classifier classes\n",
    "            if aligned_taxon_classes is None:\n",
    "                aligned_taxon_classes = classifier.classes_\n",
    "                \n",
    "                # Get the local output path for the file\n",
    "                taxon_classes_local_output_path = taxon_classes_local_output_path_template\n",
    "                \n",
    "                # Write each taxon class to the file\n",
    "                with open(taxon_classes_local_output_path, 'w') as fh:\n",
    "                    for taxon_class in aligned_taxon_classes:\n",
    "                        fh.write(taxon_class + '\\n')\n",
    "                \n",
    "                # Read the file back, for reproducibility\n",
    "                with open(taxon_classes_local_output_path, 'r') as fh:\n",
    "                    # Read all lines from the file\n",
    "                    aligned_taxon_classes = [line.strip() for line in fh.readlines()]\n",
    "                \n",
    "            else:\n",
    "                assert np.array_equal(classifier.classes_, aligned_taxon_classes)\n",
    "            \n",
    "            \n",
    "            print(' - Getting encoded genotypes')\n",
    "\n",
    "            # Get the encoded genotypes narr for the target samples for this partition\n",
    "            # Also get the aligned samples DataFrame\n",
    "            genotypes_na, aligned_samples_df = get_encoded_genotypes_na(\n",
    "                diploid_genotype_encodings=diploid_genotype_encodings,\n",
    "                genomic_positions_zarr=genomic_positions_zarr,\n",
    "                gcs=gcs,\n",
    "                snp_genotypes_gcs_path_template=snp_genotypes_gcs_path_template,\n",
    "                samples_df=derived_samples_df,\n",
    "                partition_tuple=partition_tuple,\n",
    "                warn_missing_genotypes=False\n",
    "            )\n",
    "\n",
    "            # Check the returned number of aligned samples corresponds to the genotypes arr (n_samples, n_genotypes)\n",
    "            assert len(aligned_samples_df) == genotypes_na.shape[0]\n",
    "\n",
    "            # Check the consistency of the number of samples\n",
    "            if samples_count is None:\n",
    "                samples_count = len(aligned_samples_df)\n",
    "            else:\n",
    "                assert len(aligned_samples_df) == samples_count\n",
    "\n",
    "            # Check that aligned_samples_df is constent with previous runs\n",
    "            if consistent_aligned_samples_df is None:\n",
    "                consistent_aligned_samples_df = aligned_samples_df\n",
    "            else:\n",
    "                assert aligned_samples_df.equals(consistent_aligned_samples_df)\n",
    "\n",
    "            print(' - Getting predicted class probabilities')\n",
    "            #Â This should have the same (samples, classes)\n",
    "            #Â This should return results in the same order as consistent_aligned_samples_df (and genotypes_na)\n",
    "            classifier_predicted_probs_arr = get_predicted_probs_via_classifier(classifier=classifier, genotypes_arr=genotypes_na)\n",
    "\n",
    "            # Check the consistency of the number of samples and classes\n",
    "            assert classifier_predicted_probs_arr.shape[0] == samples_count\n",
    "            assert classifier_predicted_probs_arr.shape[1] == len(aligned_taxon_classes)\n",
    "\n",
    "            print(' - Making a DataFrame')\n",
    "\n",
    "            ## Get the predicted probabilities for this partition as a DataFrame\n",
    "\n",
    "            # Get a copy of the aligned sample ids (release_str, sample_set, sample_id)\n",
    "            copy_of_aligned_samples_df = consistent_aligned_samples_df.copy().reset_index()\n",
    "\n",
    "            # Make a record of the partition tuple for every sample (contig, start_pos, stop_pos)\n",
    "            partition_df = pd.DataFrame(\n",
    "                {'contig': contig, 'start_pos': start_pos, 'stop_pos': stop_pos},\n",
    "                index=range(len(copy_of_aligned_samples_df))\n",
    "            )\n",
    "\n",
    "            # Get the list of column names for each taxon probability\n",
    "            aligned_taxon_prob_columns = [\n",
    "                f'{taxon}_prob' for taxon in aligned_taxon_classes\n",
    "            ]\n",
    "\n",
    "            # Convert the predicted probabilities array for this partition to a DataFrame\n",
    "            classifier_predicted_probs_df = pd.DataFrame(\n",
    "                classifier_predicted_probs_arr,\n",
    "                columns=aligned_taxon_prob_columns\n",
    "            )\n",
    "\n",
    "            # Concatenate the sub-DataFrames together into one, for this partition\n",
    "            predicted_probs_df = pd.concat(\n",
    "                [copy_of_aligned_samples_df, partition_df, classifier_predicted_probs_df],\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            # Add the DataFrame of predicted probabilities for this partition to the list\n",
    "            predicted_prob_dfs_per_partition.append(predicted_probs_df)\n",
    "\n",
    "            \n",
    "    ## Get a DataFrame of all predicted probs\n",
    "            \n",
    "    # Check that the number of samples in the predicted_prob DataFrames is consistent\n",
    "    assert len(predicted_prob_dfs_per_partition[0]) == len(consistent_aligned_samples_df)\n",
    "\n",
    "    # Concatenate all of the predicted_prob DataFrames\n",
    "    all_predicted_probs_df = pd.concat(predicted_prob_dfs_per_partition)\n",
    "    \n",
    "    # Set the key columns as indexes\n",
    "    all_predicted_probs_df = all_predicted_probs_df.set_index(\n",
    "        ['release_str', 'sample_set', 'sample_id', 'contig', 'start_pos', 'stop_pos']\n",
    "    )\n",
    "    \n",
    "    return all_predicted_probs_df, aligned_taxon_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "79894744-f8bd-4232-9941-1711a16f44ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputs_already_exist(*, sample_set):\n",
    "    \n",
    "    # Get the expected output paths\n",
    "    predicted_taxa_probs_output_gcs_path = predicted_taxa_probs_output_gcs_path_template.format(sample_set=sample_set)\n",
    "    provisional_taxa_output_path = provisional_taxa_output_path_template.format(\n",
    "        sample_set=sample_set\n",
    "    )\n",
    "    \n",
    "    # Determine whether all the outputs exist\n",
    "    if gcs.exists(predicted_taxa_probs_output_gcs_path) and Path(provisional_taxa_output_path).exists():\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53c92cc-9c40-4871-b8f8-94190b008109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24278420-b9f3-4d22-aa03-6cd0b44b4147",
   "metadata": {},
   "source": [
    "## Get a DataFrame of the samples that we want to run through the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a3d00f90-003a-4104-b596-479547b3eafb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "release_strings_tuple = (release_version,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f8bf4290-b292-4a2e-8810-7f482c94d380",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>release_str</th>\n",
       "      <th>sample_set</th>\n",
       "      <th>sample_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">v3.13</th>\n",
       "      <th rowspan=\"11\" valign=\"top\">1324-VO-ET-GOLASSA-VMF00257</th>\n",
       "      <th>VBS83156-7466STDY14206595</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBS83157-7466STDY14206596</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBS83158-7466STDY14206597</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBS83159-7466STDY14206598</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBS83160-7466STDY14206599</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBS83580-7466STDY14207410</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBS83581-7466STDY14207411</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBS83582-7466STDY14207412</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBS83583-7466STDY14207413</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBS83584-7466STDY14207414</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>425 rows Ã 0 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [(v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83156-7466STDY14206595), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83157-7466STDY14206596), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83158-7466STDY14206597), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83159-7466STDY14206598), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83160-7466STDY14206599), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83161-7466STDY14206600), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83162-7466STDY14206601), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83163-7466STDY14206602), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83164-7466STDY14206603), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83165-7466STDY14206604), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83166-7466STDY14206605), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83167-7466STDY14206606), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83168-7466STDY14206607), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83169-7466STDY14206608), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83170-7466STDY14206609), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83171-7466STDY14206610), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83172-7466STDY14206611), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83173-7466STDY14206612), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83174-7466STDY14206613), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83175-7466STDY14206614), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83176-7466STDY14206615), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83177-7466STDY14206616), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83178-7466STDY14206617), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83179-7466STDY14206618), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83180-7466STDY14206619), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83181-7466STDY14206620), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83182-7466STDY14206621), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83183-7466STDY14206622), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83184-7466STDY14206623), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83185-7466STDY14206624), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83186-7466STDY14206625), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83187-7466STDY14206626), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83188-7466STDY14206627), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83189-7466STDY14206628), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83190-7466STDY14206629), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83192-7466STDY14206631), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83193-7466STDY14206632), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83194-7466STDY14206633), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83195-7466STDY14206634), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83196-7466STDY14206635), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83197-7466STDY14206636), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83198-7466STDY14206637), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83199-7466STDY14206638), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83200-7466STDY14206639), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83201-7466STDY14206640), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83202-7466STDY14206641), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83203-7466STDY14206642), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83204-7466STDY14206643), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83205-7466STDY14206644), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83206-7466STDY14206691), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83207-7466STDY14206692), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83208-7466STDY14206693), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83209-7466STDY14206694), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83210-7466STDY14206695), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83211-7466STDY14206696), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83212-7466STDY14206697), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83213-7466STDY14206698), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83214-7466STDY14206699), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83215-7466STDY14206700), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83216-7466STDY14206701), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83217-7466STDY14206702), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83218-7466STDY14206703), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83219-7466STDY14206704), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83220-7466STDY14206705), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83221-7466STDY14206706), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83222-7466STDY14206707), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83223-7466STDY14206708), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83224-7466STDY14206709), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83225-7466STDY14206710), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83226-7466STDY14206711), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83227-7466STDY14206712), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83228-7466STDY14206713), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83229-7466STDY14206714), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83230-7466STDY14206715), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83231-7466STDY14206716), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83232-7466STDY14206717), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83233-7466STDY14206718), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83234-7466STDY14206719), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83235-7466STDY14206720), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83236-7466STDY14206721), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83237-7466STDY14206722), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83238-7466STDY14206723), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83239-7466STDY14206724), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83240-7466STDY14206725), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83241-7466STDY14206726), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83242-7466STDY14206727), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83243-7466STDY14206728), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83244-7466STDY14206729), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83245-7466STDY14206730), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83246-7466STDY14206731), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83247-7466STDY14206732), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83248-7466STDY14206733), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83249-7466STDY14206734), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83250-7466STDY14206735), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83251-7466STDY14206736), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83252-7466STDY14206737), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83253-7466STDY14206738), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83254-7466STDY14206739), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83255-7466STDY14206740), (v3.13, 1324-VO-ET-GOLASSA-VMF00257, VBS83256-7466STDY14206787), ...]\n",
       "\n",
       "[425 rows x 0 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derived_samples_df = get_derived_samples_df(release_strings_tuple=release_strings_tuple)\n",
    "derived_samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "08d7b1c5-f683-4e89-b5a0-8d510a330b90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_set\n",
       "1324-VO-ET-GOLASSA-VMF00257    425\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derived_samples_df.index.get_level_values('sample_set').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e10f7e-9d39-48b6-beec-6219b3f9cbcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff4c8762-98c1-49f8-b374-9fa3e2a641d3",
   "metadata": {},
   "source": [
    "## Get the genomic positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e0e3a02f-c214-4b89-a605-f51b80c753bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "genomic_positions_zarr = open_gcs_zip_zarr(gcs_url=allsites_zip_path, gcs=gcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b88a8ef-3747-47ce-80a4-8febf6c0611c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "493b1ddd-8820-4ff4-bebf-69ef59725b97",
   "metadata": {},
   "source": [
    "## Get the diploid genotype encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "30cd1811-f306-4e25-ac71-a19b82a2e54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diploid_genotype_encodings = get_diploid_genotype_encodings(\n",
    "    gcs=gcs,\n",
    "    diploid_genotype_encodings_gcs_path=diploid_genotype_encodings_gcs_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86af01e1-c2e2-45b9-93db-8e1c8150837c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "978f2c7f-374f-4441-92a3-3ee9667b7512",
   "metadata": {},
   "source": [
    "## Main process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dafcbd54-8402-4f3f-82bd-b8f84dda6d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1324-VO-ET-GOLASSA-VMF00257\n",
      "\n",
      "X\n",
      "- Partition 0 999999\n",
      " - Time 11:54:40\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 1000000 1999999\n",
      " - Time 11:56:50\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 2000000 2999999\n",
      " - Time 11:58:41\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 3000000 3999999\n",
      " - Time 12:00:31\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 4000000 4999999\n",
      " - Time 12:02:28\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 5000000 5999999\n",
      " - Time 12:04:22\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 6000000 6999999\n",
      " - Time 12:06:14\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 7000000 7999999\n",
      " - Time 12:08:07\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 9000000 9999999\n",
      " - Time 12:11:52\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 10000000 10999999\n",
      " - Time 12:13:44\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 11000000 11999999\n",
      " - Time 12:15:35\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 12000000 12999999\n",
      " - Time 12:17:34\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 13000000 13999999\n",
      " - Time 12:19:24\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 14000000 14999999\n",
      " - Time 12:21:14\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 15000000 15999999\n",
      " - Time 12:23:12\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 16000000 16999999\n",
      " - Time 12:25:01\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 17000000 17999999\n",
      " - Time 12:26:51\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 18000000 18999999\n",
      " - Time 12:28:42\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 19000000 19999999\n",
      " - Time 12:30:32\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 20000000 20999999\n",
      " - Time 12:32:22\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 21000000 21999999\n",
      " - Time 12:34:00\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 22000000 22999999\n",
      " - Time 12:35:34\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 23000000 23999999\n",
      " - Time 12:37:12\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 24000000 24393108\n",
      " - Time 12:38:45\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "\n",
      "2R\n",
      "- Partition 0 999999\n",
      " - Time 12:39:39\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 1000000 1999999\n",
      " - Time 12:41:32\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 2000000 2999999\n",
      " - Time 12:43:24\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 3000000 3999999\n",
      " - Time 12:45:16\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 4000000 4999999\n",
      " - Time 12:47:11\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 5000000 5999999\n",
      " - Time 12:49:02\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 6000000 6999999\n",
      " - Time 12:50:53\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 7000000 7999999\n",
      " - Time 12:52:45\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 8000000 8999999\n",
      " - Time 12:54:36\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 9000000 9999999\n",
      " - Time 12:56:27\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 10000000 10999999\n",
      " - Time 12:58:19\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 11000000 11999999\n",
      " - Time 13:00:09\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 12000000 12999999\n",
      " - Time 13:02:00\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 13000000 13999999\n",
      " - Time 13:03:52\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 14000000 14999999\n",
      " - Time 13:05:43\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 15000000 15999999\n",
      " - Time 13:07:33\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 16000000 16999999\n",
      " - Time 13:09:24\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 17000000 17999999\n",
      " - Time 13:11:15\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 18000000 18999999\n",
      " - Time 13:13:06\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 19000000 19999999\n",
      " - Time 13:14:57\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 20000000 20999999\n",
      " - Time 13:16:51\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 21000000 21999999\n",
      " - Time 13:18:42\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 22000000 22999999\n",
      " - Time 13:20:35\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 23000000 23999999\n",
      " - Time 13:22:26\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 24000000 24999999\n",
      " - Time 13:24:17\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 25000000 25999999\n",
      " - Time 13:26:09\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 26000000 26999999\n",
      " - Time 13:28:22\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 27000000 27999999\n",
      " - Time 13:30:21\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 28000000 28999999\n",
      " - Time 13:32:15\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 29000000 29999999\n",
      " - Time 13:34:07\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 30000000 30999999\n",
      " - Time 13:36:02\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 31000000 31999999\n",
      " - Time 13:38:12\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 32000000 32999999\n",
      " - Time 13:40:02\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 33000000 33999999\n",
      " - Time 13:41:52\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 34000000 34999999\n",
      " - Time 13:43:51\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 35000000 35999999\n",
      " - Time 13:45:44\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 36000000 36999999\n",
      " - Time 13:47:39\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 37000000 37999999\n",
      " - Time 13:49:28\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 38000000 38999999\n",
      " - Time 13:51:14\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 39000000 39999999\n",
      " - Time 13:53:05\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 40000000 40999999\n",
      " - Time 13:54:59\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 41000000 41999999\n",
      " - Time 13:56:52\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 42000000 42999999\n",
      " - Time 13:58:40\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 43000000 43999999\n",
      " - Time 14:00:30\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 44000000 44999999\n",
      " - Time 14:02:19\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 45000000 45999999\n",
      " - Time 14:04:10\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 46000000 46999999\n",
      " - Time 14:06:11\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 47000000 47999999\n",
      " - Time 14:08:03\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 48000000 48999999\n",
      " - Time 14:09:55\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 49000000 49999999\n",
      " - Time 14:11:46\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 50000000 50999999\n",
      " - Time 14:13:35\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 51000000 51999999\n",
      " - Time 14:15:40\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 52000000 52999999\n",
      " - Time 14:17:52\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 53000000 53999999\n",
      " - Time 14:19:50\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 54000000 54999999\n",
      " - Time 14:21:42\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 55000000 55999999\n",
      " - Time 14:23:36\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 56000000 56999999\n",
      " - Time 14:25:39\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 57000000 57999999\n",
      " - Time 14:27:48\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 58000000 58999999\n",
      " - Time 14:29:39\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 59000000 59999999\n",
      " - Time 14:31:32\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 60000000 60999999\n",
      " - Time 14:33:19\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 61000000 61545105\n",
      " - Time 14:34:57\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "\n",
      "2L\n",
      "- Partition 0 999999\n",
      " - Time 14:36:04\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 1000000 1999999\n",
      " - Time 14:38:02\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 2000000 2999999\n",
      " - Time 14:39:49\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 3000000 3999999\n",
      " - Time 14:41:40\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 4000000 4999999\n",
      " - Time 14:43:37\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 5000000 5999999\n",
      " - Time 14:45:29\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 6000000 6999999\n",
      " - Time 14:47:29\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 7000000 7999999\n",
      " - Time 14:49:30\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 8000000 8999999\n",
      " - Time 14:51:24\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 9000000 9999999\n",
      " - Time 14:53:11\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 10000000 10999999\n",
      " - Time 14:55:02\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 11000000 11999999\n",
      " - Time 14:56:59\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 12000000 12999999\n",
      " - Time 14:59:21\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 13000000 13999999\n",
      " - Time 15:01:26\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 14000000 14999999\n",
      " - Time 15:03:18\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 15000000 15999999\n",
      " - Time 15:05:15\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 16000000 16999999\n",
      " - Time 15:07:11\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 17000000 17999999\n",
      " - Time 15:09:24\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 18000000 18999999\n",
      " - Time 15:11:26\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 19000000 19999999\n",
      " - Time 15:13:25\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 20000000 20999999\n",
      " - Time 15:15:22\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 21000000 21999999\n",
      " - Time 15:17:16\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 22000000 22999999\n",
      " - Time 15:19:33\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 23000000 23999999\n",
      " - Time 15:21:31\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 24000000 24999999\n",
      " - Time 15:23:27\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 25000000 25999999\n",
      " - Time 15:25:27\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 26000000 26999999\n",
      " - Time 15:27:25\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 27000000 27999999\n",
      " - Time 15:29:33\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 28000000 28999999\n",
      " - Time 15:31:30\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 29000000 29999999\n",
      " - Time 15:33:28\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 30000000 30999999\n",
      " - Time 15:35:25\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 31000000 31999999\n",
      " - Time 15:37:16\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 32000000 32999999\n",
      " - Time 15:39:22\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 33000000 33999999\n",
      " - Time 15:41:20\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 34000000 34999999\n",
      " - Time 15:43:17\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 35000000 35999999\n",
      " - Time 15:45:19\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 36000000 36999999\n",
      " - Time 15:47:15\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 37000000 37999999\n",
      " - Time 15:49:13\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 38000000 38999999\n",
      " - Time 15:51:29\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 39000000 39999999\n",
      " - Time 15:53:32\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 40000000 40999999\n",
      " - Time 15:55:26\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 41000000 41999999\n",
      " - Time 15:57:24\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 42000000 42999999\n",
      " - Time 15:59:21\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 43000000 43999999\n",
      " - Time 16:01:34\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 44000000 44999999\n",
      " - Time 16:03:33\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 45000000 45999999\n",
      " - Time 16:05:29\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 46000000 46999999\n",
      " - Time 16:07:27\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 47000000 47999999\n",
      " - Time 16:09:24\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 48000000 48999999\n",
      " - Time 16:11:35\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 49000000 49364325\n",
      " - Time 16:13:32\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "\n",
      "3R\n",
      "- Partition 0 999999\n",
      " - Time 16:14:31\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 1000000 1999999\n",
      " - Time 16:16:39\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 2000000 2999999\n",
      " - Time 16:18:40\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 3000000 3999999\n",
      " - Time 16:21:08\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 4000000 4999999\n",
      " - Time 16:23:06\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 5000000 5999999\n",
      " - Time 16:25:04\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 6000000 6999999\n",
      " - Time 16:27:03\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 7000000 7999999\n",
      " - Time 16:29:05\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 8000000 8999999\n",
      " - Time 16:31:00\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 9000000 9999999\n",
      " - Time 16:33:02\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 10000000 10999999\n",
      " - Time 16:34:55\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 11000000 11999999\n",
      " - Time 16:36:51\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 12000000 12999999\n",
      " - Time 16:38:43\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 13000000 13999999\n",
      " - Time 16:41:04\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 14000000 14999999\n",
      " - Time 16:43:07\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 15000000 15999999\n",
      " - Time 16:45:06\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 16000000 16999999\n",
      " - Time 16:47:03\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 17000000 17999999\n",
      " - Time 16:49:00\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 18000000 18999999\n",
      " - Time 16:50:59\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 19000000 19999999\n",
      " - Time 16:52:50\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 20000000 20999999\n",
      " - Time 16:54:47\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 21000000 21999999\n",
      " - Time 16:56:38\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 22000000 22999999\n",
      " - Time 16:58:33\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 23000000 23999999\n",
      " - Time 17:00:50\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 24000000 24999999\n",
      " - Time 17:02:44\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 25000000 25999999\n",
      " - Time 17:04:37\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 26000000 26999999\n",
      " - Time 17:06:32\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 27000000 27999999\n",
      " - Time 17:08:25\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 28000000 28999999\n",
      " - Time 17:10:27\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 29000000 29999999\n",
      " - Time 17:12:18\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 30000000 30999999\n",
      " - Time 17:14:09\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 31000000 31999999\n",
      " - Time 17:16:02\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 32000000 32999999\n",
      " - Time 17:17:53\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 33000000 33999999\n",
      " - Time 17:19:42\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 34000000 34999999\n",
      " - Time 17:21:32\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 35000000 35999999\n",
      " - Time 17:23:23\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 36000000 36999999\n",
      " - Time 17:25:15\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 37000000 37999999\n",
      " - Time 17:27:06\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 38000000 38999999\n",
      " - Time 17:28:58\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 39000000 39999999\n",
      " - Time 17:30:48\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 40000000 40999999\n",
      " - Time 17:32:40\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 41000000 41999999\n",
      " - Time 17:34:34\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 42000000 42999999\n",
      " - Time 17:36:26\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 43000000 43999999\n",
      " - Time 17:38:19\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 44000000 44999999\n",
      " - Time 17:40:09\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 45000000 45999999\n",
      " - Time 17:42:01\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 46000000 46999999\n",
      " - Time 17:43:49\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 47000000 47999999\n",
      " - Time 17:45:38\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 48000000 48999999\n",
      " - Time 17:47:24\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 49000000 49999999\n",
      " - Time 17:49:12\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 50000000 50999999\n",
      " - Time 17:51:00\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 51000000 51999999\n",
      " - Time 17:52:50\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 52000000 52999999\n",
      " - Time 17:54:40\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 53000000 53200684\n",
      " - Time 17:56:12\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "\n",
      "3L\n",
      "- Partition 0 999999\n",
      " - Time 17:56:41\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 1000000 1999999\n",
      " - Time 17:58:11\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 2000000 2999999\n",
      " - Time 17:59:53\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 3000000 3999999\n",
      " - Time 18:01:41\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 4000000 4999999\n",
      " - Time 18:03:30\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 5000000 5999999\n",
      " - Time 18:05:08\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 6000000 6999999\n",
      " - Time 18:07:00\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 7000000 7999999\n",
      " - Time 18:08:49\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 8000000 8999999\n",
      " - Time 18:10:35\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 9000000 9999999\n",
      " - Time 18:12:26\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 10000000 10999999\n",
      " - Time 18:14:15\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 11000000 11999999\n",
      " - Time 18:16:05\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 12000000 12999999\n",
      " - Time 18:17:52\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 13000000 13999999\n",
      " - Time 18:19:44\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 14000000 14999999\n",
      " - Time 18:21:35\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 15000000 15999999\n",
      " - Time 18:23:26\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 16000000 16999999\n",
      " - Time 18:25:15\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 17000000 17999999\n",
      " - Time 18:27:06\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 18000000 18999999\n",
      " - Time 18:28:58\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 19000000 19999999\n",
      " - Time 18:30:50\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 20000000 20999999\n",
      " - Time 18:32:42\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 21000000 21999999\n",
      " - Time 18:34:34\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 22000000 22999999\n",
      " - Time 18:36:24\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 23000000 23999999\n",
      " - Time 18:38:17\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 24000000 24999999\n",
      " - Time 18:40:06\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 25000000 25999999\n",
      " - Time 18:41:56\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 26000000 26999999\n",
      " - Time 18:43:45\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 27000000 27999999\n",
      " - Time 18:45:33\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 28000000 28999999\n",
      " - Time 18:47:26\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 29000000 29999999\n",
      " - Time 18:49:18\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 30000000 30999999\n",
      " - Time 18:51:09\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 31000000 31999999\n",
      " - Time 18:53:01\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 32000000 32999999\n",
      " - Time 18:54:53\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 33000000 33999999\n",
      " - Time 18:56:45\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 34000000 34999999\n",
      " - Time 18:58:37\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 35000000 35999999\n",
      " - Time 19:00:30\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 36000000 36999999\n",
      " - Time 19:02:23\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 37000000 37999999\n",
      " - Time 19:04:12\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 38000000 38999999\n",
      " - Time 19:05:59\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 39000000 39999999\n",
      " - Time 19:07:48\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 40000000 40999999\n",
      " - Time 19:09:39\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "- Partition 41000000 41963435\n",
      " - Time 19:11:32\n",
      " - Getting classifier\n",
      " - Getting encoded genotypes\n",
      " - Getting predicted class probabilities\n",
      " - Making a DataFrame\n",
      "agg_diff_df len 0\n"
     ]
    }
   ],
   "source": [
    "# Collect DataFrame containing samples that diff in aggregates, i.e. max_votes_taxon != max_mean_prob_taxon\n",
    "agg_diff_df_by_sample_set = {}\n",
    "\n",
    "# Group by sample_set and loop over each group\n",
    "for sample_set, sample_set_group_df in derived_samples_df.groupby('sample_set'):\n",
    "    \n",
    "    print()\n",
    "    print(sample_set)\n",
    "    \n",
    "    # Determine whether the outputs for this sample_set already exist, then skip\n",
    "    if outputs_already_exist(sample_set=sample_set):\n",
    "        print('- WARNING: outputs already exist. Skipping.')\n",
    "        \n",
    "        # Get the path\n",
    "        provisional_taxa_output_path = provisional_taxa_output_path_template.format(\n",
    "            sample_set=sample_set\n",
    "        )\n",
    "        \n",
    "        # Get the aggregated data\n",
    "        samples_taxon_prob_agg_df_import = pd.read_csv(provisional_taxa_output_path, sep='\\t')\n",
    "        \n",
    "        # Compare max_votes_taxon with max_mean_prob_taxon\n",
    "        agg_diff_df = samples_taxon_prob_agg_df_import[samples_taxon_prob_agg_df_import['max_votes_taxon'] != samples_taxon_prob_agg_df_import['max_mean_prob_taxon']]\n",
    "        print('- agg_diff_df len', len(agg_diff_df))\n",
    "        \n",
    "        # Collect the differences\n",
    "        agg_diff_df_by_sample_set[sample_set] = agg_diff_df\n",
    "        \n",
    "        # Skip to the next sample_set\n",
    "        continue\n",
    "    \n",
    "    # Determine whether snp_genotypes are missing for this sample_set, then skip \n",
    "    try:\n",
    "        # Get a DataFrame of the predicted taxon probs for each partition.\n",
    "        # Also get the list of aligned taxon classes.\n",
    "        all_predicted_probs_df, aligned_taxon_classes = get_predicted_probs_df(derived_samples_df=sample_set_group_df)\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        print('- WARNING: Skipping.')\n",
    "        \n",
    "        # Skip to the next sample_set\n",
    "        continue\n",
    "    \n",
    "    # Add the max_taxon column to the DataFrame using the function\n",
    "    all_predicted_probs_df['max_taxon'] = all_predicted_probs_df.apply(\n",
    "        get_max_taxon_from_df_row,\n",
    "        taxon_classes=aligned_taxon_classes,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Add the max_taxon_prob_diff column using the function\n",
    "    all_predicted_probs_df['max_taxon_prob_diff'] = all_predicted_probs_df.apply(\n",
    "        get_max_taxon_prob_diff_from_df_row,\n",
    "        taxon_classes=aligned_taxon_classes,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Add the max_taxon_prob_qual column using the function\n",
    "    all_predicted_probs_df['max_taxon_prob_qual'] = all_predicted_probs_df.apply(\n",
    "        get_max_taxon_prob_qual_from_df_row,\n",
    "        taxon_classes=aligned_taxon_classes,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Export the DataFrame of all the predicated probs for this sample set\n",
    "    predicted_taxa_probs_output_gcs_path = predicted_taxa_probs_output_gcs_path_template.format(sample_set=sample_set)\n",
    "    with gcs.open(predicted_taxa_probs_output_gcs_path, 'w') as fh:\n",
    "        all_predicted_probs_df.to_csv(fh, sep='\\t', index=True)\n",
    "    \n",
    "    # Reload for reproducibility\n",
    "    with gcs.open(predicted_taxa_probs_output_gcs_path, 'r') as fh:\n",
    "        all_predicted_probs_df = pd.read_csv(\n",
    "            fh,\n",
    "            sep='\\t',\n",
    "            index_col=['release_str', 'sample_set', 'sample_id', 'contig', 'start_pos', 'stop_pos']\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # Get the averages for each taxon probability for each sample over all partitions\n",
    "    samples_taxon_prob_averages_df = get_samples_taxon_prob_averages_df(\n",
    "        taxon_probs_by_partition_df=all_predicted_probs_df,\n",
    "        taxon_classes=aligned_taxon_classes\n",
    "    )\n",
    "    \n",
    "    # Add the max_mean_prob_taxon column to the DataFrame using the function\n",
    "    samples_taxon_prob_averages_df['max_mean_prob_taxon'] = samples_taxon_prob_averages_df.apply(\n",
    "        get_max_mean_prob_taxon_from_df_row,\n",
    "        taxon_classes=aligned_taxon_classes,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Add the max_mean_taxon_prob_diff column using the function\n",
    "    samples_taxon_prob_averages_df['max_mean_taxon_prob_diff'] = samples_taxon_prob_averages_df.apply(\n",
    "        get_max_mean_prob_taxon_diff_from_df_row,\n",
    "        taxon_classes=aligned_taxon_classes,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Add the max_mean_taxon_prob_qual column using the function\n",
    "    samples_taxon_prob_averages_df['max_mean_taxon_prob_qual'] = samples_taxon_prob_averages_df.apply(\n",
    "        get_max_mean_prob_taxon_qual_from_df_row,\n",
    "        taxon_classes=aligned_taxon_classes,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Get the max_taxon value counts for each sample over all partitions  \n",
    "    samples_max_taxon_counts_df = get_samples_max_taxon_counts_df(\n",
    "        taxon_probs_by_partition_df=all_predicted_probs_df,\n",
    "        taxon_classes=aligned_taxon_classes\n",
    "    )\n",
    "    \n",
    "    # Add the max_votes_taxon column to the DataFrame using the function\n",
    "    samples_max_taxon_counts_df['max_votes_taxon'] = samples_max_taxon_counts_df.apply(\n",
    "        get_max_votes_taxon_from_df_row,\n",
    "        taxon_classes=aligned_taxon_classes,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Add the max_votes_taxon_ratio_diff column using the function\n",
    "    samples_max_taxon_counts_df['max_votes_taxon_ratio_diff'] = samples_max_taxon_counts_df.apply(\n",
    "        get_max_votes_taxon_ratio_diff_from_df_row,\n",
    "        taxon_classes=aligned_taxon_classes,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Merge the probability averages DataFrame with the votes DataFrame\n",
    "    samples_taxon_prob_agg_df = samples_taxon_prob_averages_df.merge(samples_max_taxon_counts_df, left_index=True, right_index=True)\n",
    "    \n",
    "    # Compare max_votes_taxon with max_mean_prob_taxon\n",
    "    agg_diff_df = samples_taxon_prob_agg_df[samples_taxon_prob_agg_df['max_votes_taxon'] != samples_taxon_prob_agg_df['max_mean_prob_taxon']]\n",
    "    print('agg_diff_df len', len(agg_diff_df))\n",
    "    \n",
    "    # Collect the differences\n",
    "    agg_diff_df_by_sample_set[sample_set] = agg_diff_df\n",
    "    \n",
    "    \n",
    "    ## Export the aggregated provisional taxon data for this sample set\n",
    "    \n",
    "    # Convert the indexes to columns\n",
    "    samples_taxon_prob_agg_df_export = samples_taxon_prob_agg_df.reset_index()\n",
    "    \n",
    "    # Drop the release_str and sample_set columns\n",
    "    samples_taxon_prob_agg_df_export.drop(columns=['release_str', 'sample_set'], inplace=True)\n",
    "    \n",
    "    # Rename the sample_id column to derived_sample_id\n",
    "    samples_taxon_prob_agg_df_export.rename(columns={'sample_id': 'derived_sample_id'}, inplace=True)\n",
    "    \n",
    "    # Get the path\n",
    "    provisional_taxa_output_path = provisional_taxa_output_path_template.format(\n",
    "        sample_set=sample_set\n",
    "    )\n",
    "    \n",
    "    # Export the aggregated data from the DataFrame to a TSV file\n",
    "    samples_taxon_prob_agg_df_export.to_csv(provisional_taxa_output_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16fc125-620b-481c-803c-1eb316f1f312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "60ced115-8c53-445e-bd9b-24767e379536",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1324-VO-ET-GOLASSA-VMF00257'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exemplar_sample_set = list(agg_diff_df_by_sample_set.keys())[0]\n",
    "exemplar_sample_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "271a6900-35ec-4183-9ba7-342a2b7887b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>max_mean_prob_taxon</th>\n",
       "      <th>max_votes_taxon</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>release_str</th>\n",
       "      <th>sample_set</th>\n",
       "      <th>sample_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [max_mean_prob_taxon, max_votes_taxon]\n",
       "Index: []"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_diff_df_by_sample_set[exemplar_sample_set][['max_mean_prob_taxon', 'max_votes_taxon']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbc826e-d23c-4609-bf33-074aa7fd47ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global-global-mgenv-5.1.0",
   "language": "python",
   "name": "conda-env-global-global-mgenv-5.1.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
